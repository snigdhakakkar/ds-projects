{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('datasets/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                 0\n",
       "MSSubClass         0\n",
       "MSZoning           0\n",
       "LotFrontage      259\n",
       "LotArea            0\n",
       "                ... \n",
       "MoSold             0\n",
       "YrSold             0\n",
       "SaleType           0\n",
       "SaleCondition      0\n",
       "SalePrice          0\n",
       "Length: 81, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df.isnull(), yticklabels=False, cbar= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1460 non-null   int64  \n",
      " 1   MSSubClass     1460 non-null   int64  \n",
      " 2   MSZoning       1460 non-null   object \n",
      " 3   LotFrontage    1201 non-null   float64\n",
      " 4   LotArea        1460 non-null   int64  \n",
      " 5   Street         1460 non-null   object \n",
      " 6   Alley          91 non-null     object \n",
      " 7   LotShape       1460 non-null   object \n",
      " 8   LandContour    1460 non-null   object \n",
      " 9   Utilities      1460 non-null   object \n",
      " 10  LotConfig      1460 non-null   object \n",
      " 11  LandSlope      1460 non-null   object \n",
      " 12  Neighborhood   1460 non-null   object \n",
      " 13  Condition1     1460 non-null   object \n",
      " 14  Condition2     1460 non-null   object \n",
      " 15  BldgType       1460 non-null   object \n",
      " 16  HouseStyle     1460 non-null   object \n",
      " 17  OverallQual    1460 non-null   int64  \n",
      " 18  OverallCond    1460 non-null   int64  \n",
      " 19  YearBuilt      1460 non-null   int64  \n",
      " 20  YearRemodAdd   1460 non-null   int64  \n",
      " 21  RoofStyle      1460 non-null   object \n",
      " 22  RoofMatl       1460 non-null   object \n",
      " 23  Exterior1st    1460 non-null   object \n",
      " 24  Exterior2nd    1460 non-null   object \n",
      " 25  MasVnrType     1452 non-null   object \n",
      " 26  MasVnrArea     1452 non-null   float64\n",
      " 27  ExterQual      1460 non-null   object \n",
      " 28  ExterCond      1460 non-null   object \n",
      " 29  Foundation     1460 non-null   object \n",
      " 30  BsmtQual       1423 non-null   object \n",
      " 31  BsmtCond       1423 non-null   object \n",
      " 32  BsmtExposure   1422 non-null   object \n",
      " 33  BsmtFinType1   1423 non-null   object \n",
      " 34  BsmtFinSF1     1460 non-null   int64  \n",
      " 35  BsmtFinType2   1422 non-null   object \n",
      " 36  BsmtFinSF2     1460 non-null   int64  \n",
      " 37  BsmtUnfSF      1460 non-null   int64  \n",
      " 38  TotalBsmtSF    1460 non-null   int64  \n",
      " 39  Heating        1460 non-null   object \n",
      " 40  HeatingQC      1460 non-null   object \n",
      " 41  CentralAir     1460 non-null   object \n",
      " 42  Electrical     1459 non-null   object \n",
      " 43  1stFlrSF       1460 non-null   int64  \n",
      " 44  2ndFlrSF       1460 non-null   int64  \n",
      " 45  LowQualFinSF   1460 non-null   int64  \n",
      " 46  GrLivArea      1460 non-null   int64  \n",
      " 47  BsmtFullBath   1460 non-null   int64  \n",
      " 48  BsmtHalfBath   1460 non-null   int64  \n",
      " 49  FullBath       1460 non-null   int64  \n",
      " 50  HalfBath       1460 non-null   int64  \n",
      " 51  BedroomAbvGr   1460 non-null   int64  \n",
      " 52  KitchenAbvGr   1460 non-null   int64  \n",
      " 53  KitchenQual    1460 non-null   object \n",
      " 54  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 55  Functional     1460 non-null   object \n",
      " 56  Fireplaces     1460 non-null   int64  \n",
      " 57  FireplaceQu    770 non-null    object \n",
      " 58  GarageType     1379 non-null   object \n",
      " 59  GarageYrBlt    1379 non-null   float64\n",
      " 60  GarageFinish   1379 non-null   object \n",
      " 61  GarageCars     1460 non-null   int64  \n",
      " 62  GarageArea     1460 non-null   int64  \n",
      " 63  GarageQual     1379 non-null   object \n",
      " 64  GarageCond     1379 non-null   object \n",
      " 65  PavedDrive     1460 non-null   object \n",
      " 66  WoodDeckSF     1460 non-null   int64  \n",
      " 67  OpenPorchSF    1460 non-null   int64  \n",
      " 68  EnclosedPorch  1460 non-null   int64  \n",
      " 69  3SsnPorch      1460 non-null   int64  \n",
      " 70  ScreenPorch    1460 non-null   int64  \n",
      " 71  PoolArea       1460 non-null   int64  \n",
      " 72  PoolQC         7 non-null      object \n",
      " 73  Fence          281 non-null    object \n",
      " 74  MiscFeature    54 non-null     object \n",
      " 75  MiscVal        1460 non-null   int64  \n",
      " 76  MoSold         1460 non-null   int64  \n",
      " 77  YrSold         1460 non-null   int64  \n",
      " 78  SaleType       1460 non-null   object \n",
      " 79  SaleCondition  1460 non-null   object \n",
      " 80  SalePrice      1460 non-null   int64  \n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill Missing Values\n",
    "\n",
    "df['LotFrontage']=df['LotFrontage'].fillna(df['LotFrontage'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RL         1151\n",
       "RM          218\n",
       "FV           65\n",
       "RH           16\n",
       "C (all)      10\n",
       "Name: MSZoning, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['MSZoning'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Alley'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BsmtCond']=df['BsmtCond'].fillna(df['BsmtCond'].mode()[0])\n",
    "df['BsmtQual']=df['BsmtQual'].fillna(df['BsmtQual'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FireplaceQu']=df['FireplaceQu'].fillna(df['FireplaceQu'].mode()[0])\n",
    "df['GarageType']=df['GarageType'].fillna(df['GarageType'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['GarageYrBlt'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GarageFinish']=df['GarageFinish'].fillna(df['GarageFinish'].mode()[0])\n",
    "df['GarageQual']=df['GarageQual'].fillna(df['GarageQual'].mode()[0])\n",
    "df['GarageCond']=df['GarageCond'].fillna(df['GarageCond'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['PoolQC','Fence','MiscFeature'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I have basically dropped only thsoe features which have more than 60% of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 76)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSSubClass       0\n",
       "MSZoning         0\n",
       "LotFrontage      0\n",
       "LotArea          0\n",
       "Street           0\n",
       "                ..\n",
       "MoSold           0\n",
       "YrSold           0\n",
       "SaleType         0\n",
       "SaleCondition    0\n",
       "SalePrice        0\n",
       "Length: 75, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MasVnrType']=df['MasVnrType'].fillna(df['MasVnrType'].mode()[0])\n",
    "df['MasVnrArea']=df['MasVnrArea'].fillna(df['MasVnrArea'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BsmtExposure']=df['BsmtExposure'].fillna(df['BsmtExposure'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BsmtFinType2']=df['BsmtFinType2'].fillna(df['BsmtFinType2'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1422, 75)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape LandContour  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg         Lvl   \n",
       "1          20       RL         80.0     9600   Pave      Reg         Lvl   \n",
       "2          60       RL         68.0    11250   Pave      IR1         Lvl   \n",
       "3          70       RL         60.0     9550   Pave      IR1         Lvl   \n",
       "4          60       RL         84.0    14260   Pave      IR1         Lvl   \n",
       "\n",
       "  Utilities LotConfig LandSlope  ... EnclosedPorch 3SsnPorch ScreenPorch  \\\n",
       "0    AllPub    Inside       Gtl  ...             0         0           0   \n",
       "1    AllPub       FR2       Gtl  ...             0         0           0   \n",
       "2    AllPub    Inside       Gtl  ...             0         0           0   \n",
       "3    AllPub    Corner       Gtl  ...           272         0           0   \n",
       "4    AllPub       FR2       Gtl  ...             0         0           0   \n",
       "\n",
       "  PoolArea MiscVal  MoSold  YrSold  SaleType  SaleCondition SalePrice  \n",
       "0        0       0       2    2008        WD         Normal    208500  \n",
       "1        0       0       5    2007        WD         Normal    181500  \n",
       "2        0       0       9    2008        WD         Normal    223500  \n",
       "3        0       0       2    2006        WD        Abnorml    140000  \n",
       "4        0       0      12    2008        WD         Normal    250000  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Handle Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['MSZoning','Street','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood',\n",
    "         'Condition2','BldgType','Condition1','HouseStyle','SaleType',\n",
    "        'SaleCondition','ExterCond',\n",
    "         'ExterQual','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n",
    "        'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Heating','HeatingQC',\n",
    "         'CentralAir',\n",
    "         'Electrical','KitchenQual','Functional',\n",
    "         'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_onehot_multcols(multicolumns, final_df):\n",
    "    \n",
    "    df_final=final_df\n",
    "    \n",
    "    i=0\n",
    "    for fields in multicolumns:\n",
    "        \n",
    "        print(fields)\n",
    "        df1=pd.get_dummies(final_df[fields],drop_first=True)\n",
    "        \n",
    "        final_df.drop([fields],axis=1,inplace=True)\n",
    "        \n",
    "        if i==0:\n",
    "            df_final=df1.copy()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            df_final=pd.concat([df_final,df1],axis=1)\n",
    "            \n",
    "        i=i+1\n",
    "       \n",
    "        \n",
    "    df_final=pd.concat([final_df,df_final],axis=1)\n",
    "        \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine Test Data \n",
    "\n",
    "test_df=pd.read_csv('modifiedtest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 74)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape LandContour  \\\n",
       "0          20       RH         80.0    11622   Pave      Reg         Lvl   \n",
       "1          20       RL         81.0    14267   Pave      IR1         Lvl   \n",
       "2          60       RL         74.0    13830   Pave      IR1         Lvl   \n",
       "3          60       RL         78.0     9978   Pave      IR1         Lvl   \n",
       "4         120       RL         43.0     5005   Pave      IR1         HLS   \n",
       "\n",
       "  Utilities LotConfig LandSlope  ... OpenPorchSF EnclosedPorch 3SsnPorch  \\\n",
       "0    AllPub    Inside       Gtl  ...           0             0         0   \n",
       "1    AllPub    Corner       Gtl  ...          36             0         0   \n",
       "2    AllPub    Inside       Gtl  ...          34             0         0   \n",
       "3    AllPub    Inside       Gtl  ...          36             0         0   \n",
       "4    AllPub    Inside       Gtl  ...          82             0         0   \n",
       "\n",
       "  ScreenPorch PoolArea  MiscVal  MoSold  YrSold  SaleType SaleCondition  \n",
       "0         120        0        0       6    2010        WD        Normal  \n",
       "1           0        0    12500       6    2010        WD        Normal  \n",
       "2           0        0        0       3    2010        WD        Normal  \n",
       "3           0        0        0       6    2010        WD        Normal  \n",
       "4         144        0        0       1    2010        WD        Normal  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed that there are some categorical features in test dataset for which the categories are different from our training dataset, thus I concatenated the training and test dataset first before applying one-hot encoding for the categorical features to incorporate the difference in the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.concat([df,test_df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500.0\n",
       "1       181500.0\n",
       "2       223500.0\n",
       "3       140000.0\n",
       "4       250000.0\n",
       "          ...   \n",
       "1454         NaN\n",
       "1455         NaN\n",
       "1456         NaN\n",
       "1457         NaN\n",
       "1458         NaN\n",
       "Name: SalePrice, Length: 2881, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 75)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning\n",
      "Street\n",
      "LotShape\n",
      "LandContour\n",
      "Utilities\n",
      "LotConfig\n",
      "LandSlope\n",
      "Neighborhood\n",
      "Condition2\n",
      "BldgType\n",
      "Condition1\n",
      "HouseStyle\n",
      "SaleType\n",
      "SaleCondition\n",
      "ExterCond\n",
      "ExterQual\n",
      "Foundation\n",
      "BsmtQual\n",
      "BsmtCond\n",
      "BsmtExposure\n",
      "BsmtFinType1\n",
      "BsmtFinType2\n",
      "RoofStyle\n",
      "RoofMatl\n",
      "Exterior1st\n",
      "Exterior2nd\n",
      "MasVnrType\n",
      "Heating\n",
      "HeatingQC\n",
      "CentralAir\n",
      "Electrical\n",
      "KitchenQual\n",
      "Functional\n",
      "FireplaceQu\n",
      "GarageType\n",
      "GarageFinish\n",
      "GarageQual\n",
      "GarageCond\n",
      "PavedDrive\n"
     ]
    }
   ],
   "source": [
    "final_df=category_onehot_multcols(columns, final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 235)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate columns\n",
    "\n",
    "final_df =final_df.loc[:,~final_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 175)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          60         65.0     8450            7            5       2003   \n",
       "1          20         80.0     9600            6            8       1976   \n",
       "2          60         68.0    11250            7            5       2001   \n",
       "3          70         60.0     9550            7            5       1915   \n",
       "4          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0          2003       196.0       706.0         0.0  ...     0     0    1   \n",
       "1          1976         0.0       978.0         0.0  ...     0     0    1   \n",
       "2          2002       162.0       486.0         0.0  ...     0     0    1   \n",
       "3          1970         0.0       216.0         0.0  ...     0     0    1   \n",
       "4          2000       350.0       655.0         0.0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    1  0  \n",
       "1       1        0        0        0       0    1  0  \n",
       "2       1        0        0        0       0    1  0  \n",
       "3       0        0        0        0       1    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 175 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=final_df.iloc[:1422,:]\n",
    "df_Test=final_df.iloc[1422:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          60         65.0     8450            7            5       2003   \n",
       "1          20         80.0     9600            6            8       1976   \n",
       "2          60         68.0    11250            7            5       2001   \n",
       "3          70         60.0     9550            7            5       1915   \n",
       "4          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0          2003       196.0       706.0         0.0  ...     0     0    1   \n",
       "1          1976         0.0       978.0         0.0  ...     0     0    1   \n",
       "2          2002       162.0       486.0         0.0  ...     0     0    1   \n",
       "3          1970         0.0       216.0         0.0  ...     0     0    1   \n",
       "4          2000       350.0       655.0         0.0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    1  0  \n",
       "1       1        0        0        0       0    1  0  \n",
       "2       1        0        0        0       0    1  0  \n",
       "3       0        0        0        0       1    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 175 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1958</td>\n",
       "      <td>1958</td>\n",
       "      <td>108.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>20.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          20         80.0    11622            5            6       1961   \n",
       "1          20         81.0    14267            6            6       1958   \n",
       "2          60         74.0    13830            5            5       1997   \n",
       "3          60         78.0     9978            6            6       1998   \n",
       "4         120         43.0     5005            8            5       1992   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0          1961         0.0       468.0       144.0  ...     0     0    1   \n",
       "1          1958       108.0       923.0         0.0  ...     0     0    1   \n",
       "2          1998         0.0       791.0         0.0  ...     0     0    1   \n",
       "3          1998        20.0       602.0         0.0  ...     0     0    1   \n",
       "4          1992         0.0       263.0         0.0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    0  0  \n",
       "1       1        0        0        0       0    0  0  \n",
       "2       1        0        0        0       0    0  0  \n",
       "3       1        0        0        0       0    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 175 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1422, 175)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snigdhakakkar/opt/anaconda2/envs/tf/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "df_Test.drop(['SalePrice'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1958</td>\n",
       "      <td>1958</td>\n",
       "      <td>108.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>20.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 174 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          20         80.0    11622            5            6       1961   \n",
       "1          20         81.0    14267            6            6       1958   \n",
       "2          60         74.0    13830            5            5       1997   \n",
       "3          60         78.0     9978            6            6       1998   \n",
       "4         120         43.0     5005            8            5       1992   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0          1961         0.0       468.0       144.0  ...     0     0    1   \n",
       "1          1958       108.0       923.0         0.0  ...     0     0    1   \n",
       "2          1998         0.0       791.0         0.0  ...     0     0    1   \n",
       "3          1998        20.0       602.0         0.0  ...     0     0    1   \n",
       "4          1992         0.0       263.0         0.0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    0  0  \n",
       "1       1        0        0        0       0    0  0  \n",
       "2       1        0        0        0       0    0  0  \n",
       "3       1        0        0        0       0    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 174 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df_Train.drop(['SalePrice'],axis=1)\n",
    "y_train=df_Train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1422, 174)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediciton and selecting the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.3\n",
      "  latest version: 4.12.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/snigdhakakkar/opt/anaconda2/envs/tf\n",
      "\n",
      "  added / updated specs:\n",
      "    - py-xgboost\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _py-xgboost-mutex-2.0      |            cpu_0           8 KB  anaconda\n",
      "    libxgboost-0.90            |       hb1e8313_1         2.3 MB  anaconda\n",
      "    py-xgboost-0.90            |   py37hb1e8313_1          76 KB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         2.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _py-xgboost-mutex  anaconda/osx-64::_py-xgboost-mutex-2.0-cpu_0\n",
      "  libxgboost         anaconda/osx-64::libxgboost-0.90-hb1e8313_1\n",
      "  py-xgboost         anaconda/osx-64::py-xgboost-0.90-py37hb1e8313_1\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2022.3.29-~ --> anaconda::ca-certificates-2020.10.14-0\n",
      "  certifi            pkgs/main::certifi-2021.10.8-py37hecd~ --> anaconda::certifi-2020.6.20-py37_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "_py-xgboost-mutex-2. | 8 KB      | ##################################### | 100% \n",
      "py-xgboost-0.90      | 76 KB     | ##################################### | 100% \n",
      "libxgboost-0.90      | 2.3 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c anaconda py-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "classifier=xgboost.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "regressor=xgboost.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster=['gbtree','gblinear']\n",
    "base_score=[0.25,0.5,0.75,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper Parameter Optimization\n",
    "\n",
    "\n",
    "n_estimators = [100, 500, 900, 1100, 1500]\n",
    "max_depth = [2, 3, 5, 10, 15]\n",
    "booster=['gbtree','gblinear']\n",
    "learning_rate=[0.05,0.1,0.15,0.20]\n",
    "min_child_weight=[1,2,3,4]\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth':max_depth,\n",
    "    'learning_rate':learning_rate,\n",
    "    'min_child_weight':min_child_weight,\n",
    "    'booster':booster,\n",
    "    'base_score':base_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Set up the random search with 4-fold cross validation\n",
    "random_cv = RandomizedSearchCV(estimator=regressor,\n",
    "            param_distributions=hyperparameter_grid,\n",
    "            cv=5, n_iter=50,\n",
    "            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n",
    "            verbose = 5, \n",
    "            return_train_score = True,\n",
    "            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   48.5s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:  6.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=1500, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75 \n",
      "[15:39:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75, score=(train=-20948.916, test=-21203.830), total=  16.5s\n",
      "[CV] n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gbtree, base_score=1 \n",
      "[15:39:34] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gbtree, base_score=1, score=(train=-2170.611, test=-16890.249), total=  23.3s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[15:39:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-21244.567, test=-22291.621), total=   5.2s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[15:40:03] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-21951.430, test=-20915.133), total=   5.3s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[15:40:08] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-20742.350, test=-24818.393), total=   5.0s\n",
      "[CV] n_estimators=900, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gbtree, base_score=0.5 \n",
      "[15:40:14] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gbtree, base_score=0.5, score=(train=-19.049, test=-18041.238), total= 1.0min\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:41:14] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-3454.144, test=-17536.579), total=  23.6s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:41:38] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-3469.567, test=-18410.876), total=  24.0s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1 \n",
      "[15:42:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1, score=(train=-2498.660, test=-17826.181), total=  23.8s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=1 \n",
      "[15:42:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=1, score=(train=-10.993, test=-17602.604), total=  24.9s\n",
      "[CV] n_estimators=900, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.25 \n",
      "[15:42:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.25, score=(train=-7041.351, test=-15054.526), total=  14.2s\n",
      "[CV] n_estimators=900, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.25 \n",
      "[15:43:05] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.25, score=(train=-7281.862, test=-14395.359), total=  14.2s\n",
      "[CV] n_estimators=900, min_child_weight=3, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=0.25 \n",
      "[15:43:20] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=3, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=0.25, score=(train=-0.058, test=-17765.734), total= 1.0min\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:44:22] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-21188.382, test=-22215.758), total=   9.4s\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:44:31] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-20585.701, test=-24580.968), total=   9.3s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:44:41] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-553.508, test=-17039.758), total=  29.7s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.2, booster=gblinear, base_score=0.25 \n",
      "[15:45:10] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.2, booster=gblinear, base_score=0.25, score=(train=-21354.165, test=-21201.793), total=   5.0s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.2, booster=gblinear, base_score=0.25 \n",
      "[15:45:16] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.2, booster=gblinear, base_score=0.25, score=(train=-21170.967, test=-22187.572), total=   5.2s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.2, booster=gblinear, base_score=0.25 \n",
      "[15:45:21] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.2, booster=gblinear, base_score=0.25, score=(train=-21856.342, test=-20844.905), total=   5.2s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=3, learning_rate=0.05, booster=gblinear, base_score=0.5 \n",
      "[15:45:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=3, learning_rate=0.05, booster=gblinear, base_score=0.5, score=(train=-21710.804, test=-21157.176), total=   5.1s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=3, learning_rate=0.05, booster=gblinear, base_score=0.5 \n",
      "[15:45:31] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=3, learning_rate=0.05, booster=gblinear, base_score=0.5, score=(train=-21336.185, test=-21740.213), total=   5.2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed: 12.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=1500, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75 \n",
      "[15:39:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75, score=(train=-21338.040, test=-21200.300), total=  16.2s\n",
      "[CV] n_estimators=1500, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75 \n",
      "[15:39:34] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75, score=(train=-20540.704, test=-24540.214), total=  16.4s\n",
      "[CV] n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gbtree, base_score=1 \n",
      "[15:39:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gbtree, base_score=1, score=(train=-2280.929, test=-14493.092), total=  22.1s\n",
      "[CV] n_estimators=900, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gbtree, base_score=0.5 \n",
      "[15:40:13] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gbtree, base_score=0.5, score=(train=-14.392, test=-17727.794), total= 1.0min\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:41:14] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-3441.721, test=-15528.196), total=  24.0s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:41:38] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-3518.876, test=-14652.823), total=  23.7s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1 \n",
      "[15:42:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1, score=(train=-2505.322, test=-17032.147), total=  23.7s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=1 \n",
      "[15:42:25] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=1, score=(train=-14.124, test=-16474.656), total=  24.5s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=1 \n",
      "[15:42:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=1, score=(train=-53.662, test=-17167.393), total=  23.9s\n",
      "[CV] n_estimators=900, min_child_weight=3, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=0.25 \n",
      "[15:43:14] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=3, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=0.25, score=(train=-7.343, test=-16898.155), total= 1.0min\n",
      "[CV] n_estimators=900, min_child_weight=3, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=0.25 \n",
      "[15:44:16] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=3, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=0.25, score=(train=-0.043, test=-18227.190), total= 1.0min\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.2, booster=gblinear, base_score=0.25 \n",
      "[15:45:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.2, booster=gblinear, base_score=0.25, score=(train=-20964.484, test=-21231.190), total=   5.2s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.2, booster=gblinear, base_score=0.25 \n",
      "[15:45:23] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.2, booster=gblinear, base_score=0.25, score=(train=-20598.691, test=-24602.852), total=   5.2s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=3, learning_rate=0.05, booster=gblinear, base_score=0.5 \n",
      "[15:45:28] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=3, learning_rate=0.05, booster=gblinear, base_score=0.5, score=(train=-21490.469, test=-22617.603), total=   5.2s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=3, learning_rate=0.05, booster=gblinear, base_score=0.5 \n",
      "[15:45:33] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=3, learning_rate=0.05, booster=gblinear, base_score=0.5, score=(train=-22159.757, test=-21113.850), total=   5.2s\n",
      "[CV] n_estimators=900, min_child_weight=1, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=1 \n",
      "[15:45:39] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=1, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=1, score=(train=-0.085, test=-17862.693), total= 1.1min\n",
      "[CV] n_estimators=900, min_child_weight=4, max_depth=2, learning_rate=0.05, booster=gbtree, base_score=0.25 \n",
      "[15:46:43] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=4, max_depth=2, learning_rate=0.05, booster=gbtree, base_score=0.25, score=(train=-9318.526, test=-15442.570), total=  14.8s\n",
      "[CV] n_estimators=900, min_child_weight=4, max_depth=2, learning_rate=0.05, booster=gbtree, base_score=0.25 \n",
      "[15:46:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=4, max_depth=2, learning_rate=0.05, booster=gbtree, base_score=0.25, score=(train=-9615.460, test=-13875.231), total=  14.4s\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.5 \n",
      "[15:47:13] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.5, score=(train=-833.060, test=-17509.333), total=  18.3s\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.5 \n",
      "[15:47:31] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.5, score=(train=-921.106, test=-17899.492), total=  18.5s\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.75 \n",
      "[15:47:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.75, score=(train=-21863.409, test=-20873.256), total=  10.1s[CV] n_estimators=1500, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75 \n",
      "[15:39:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75, score=(train=-21178.815, test=-22207.231), total=  16.3s\n",
      "[CV] n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gbtree, base_score=1 \n",
      "[15:39:34] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gbtree, base_score=1, score=(train=-2160.108, test=-16988.993), total=  23.3s\n",
      "[CV] n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gbtree, base_score=1 \n",
      "[15:39:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gbtree, base_score=1, score=(train=-2223.785, test=-18277.787), total=  21.8s\n",
      "[CV] n_estimators=900, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gbtree, base_score=0.5 \n",
      "[15:40:20] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gbtree, base_score=0.5, score=(train=-13.372, test=-15207.749), total= 1.0min\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:41:21] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-3361.638, test=-16468.477), total=  23.7s\n",
      "[CV] n_estimators=100, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75 \n",
      "[15:41:44] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75, score=(train=-21572.422, test=-20916.689), total=   1.1s\n",
      "[CV] n_estimators=100, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75 \n",
      "[15:41:45] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75, score=(train=-21285.078, test=-22436.109), total=   1.0s\n",
      "[CV] n_estimators=100, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75 \n",
      "[15:41:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75, score=(train=-21141.303, test=-21741.793), total=   1.0s\n",
      "[CV] n_estimators=100, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75 \n",
      "[15:41:48] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75, score=(train=-21961.562, test=-20544.258), total=   1.1s\n",
      "[CV] n_estimators=100, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75 \n",
      "[15:41:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75, score=(train=-20944.744, test=-25323.213), total=   1.1s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1 \n",
      "[15:41:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1, score=(train=-2524.214, test=-15841.762), total=  23.8s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1 \n",
      "[15:42:14] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1, score=(train=-2664.029, test=-18473.026), total=  23.5s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=1 \n",
      "[15:42:37] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=1, score=(train=-13.367, test=-15203.087), total=  24.4s\n",
      "[CV] n_estimators=900, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.25 \n",
      "[15:43:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.25, score=(train=-6694.151, test=-15348.919), total=  14.1s\n",
      "[CV] n_estimators=900, min_child_weight=3, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=0.25 \n",
      "[15:43:16] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=3, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=0.25, score=(train=-7.345, test=-17508.270), total= 1.0min\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:44:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-21366.110, test=-21219.176), total=   9.1s\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:44:27] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-20972.337, test=-21235.545), total=   9.6s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:44:37] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-583.312, test=-16452.768), total=  29.7s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:45:07] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-555.259, test=-14316.850), total=  30.2s\n",
      "[CV] n_estimators=900, min_child_weight=1, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=1 \n",
      "[15:45:37] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=1, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=1, score=(train=-7.373, test=-17425.217), total= 1.1min\n",
      "[CV] n_estimators=900, min_child_weight=1, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=1 \n",
      "[15:46:41] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=900, min_child_weight=1, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=1, score=(train=-0.081, test=-18416.993), total= 1.1min[CV] n_estimators=1500, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75 \n",
      "[15:39:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=1, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.75, score=(train=-21832.902, test=-20848.437), total=  16.3s\n",
      "[CV] n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gbtree, base_score=1 \n",
      "[15:39:34] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gbtree, base_score=1, score=(train=-2249.678, test=-15909.905), total=  23.4s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[15:39:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-21444.106, test=-21188.814), total=   5.2s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[15:40:03] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-21064.478, test=-21355.966), total=   5.3s\n",
      "[CV] n_estimators=900, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gbtree, base_score=0.5 \n",
      "[15:40:09] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gbtree, base_score=0.5, score=(train=-14.176, test=-16117.030), total= 1.0min\n",
      "[CV] n_estimators=900, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gbtree, base_score=0.5 \n",
      "[15:41:09] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gbtree, base_score=0.5, score=(train=-33.948, test=-17765.071), total=  59.9s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1 \n",
      "[15:42:09] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1, score=(train=-2563.860, test=-14794.090), total=  23.7s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=1 \n",
      "[15:42:33] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=1, score=(train=-56.064, test=-17921.738), total=  24.4s\n",
      "[CV] n_estimators=900, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.25 \n",
      "[15:42:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.25, score=(train=-6911.767, test=-17429.128), total=  14.2s\n",
      "[CV] n_estimators=900, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.25 \n",
      "[15:43:12] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.25, score=(train=-6480.950, test=-16658.515), total=  14.1s\n",
      "[CV] n_estimators=900, min_child_weight=3, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=0.25 \n",
      "[15:43:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=3, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=0.25, score=(train=-7.357, test=-15756.195), total= 1.0min\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:44:28] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-21863.406, test=-20873.253), total=   9.4s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:44:37] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-502.151, test=-16831.661), total=  29.6s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:45:07] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-545.985, test=-17542.390), total=  29.9s\n",
      "[CV] n_estimators=900, min_child_weight=1, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=1 \n",
      "[15:45:37] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=1, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=1, score=(train=-7.362, test=-17015.288), total= 1.1min\n",
      "[CV] n_estimators=900, min_child_weight=4, max_depth=2, learning_rate=0.05, booster=gbtree, base_score=0.25 \n",
      "[15:46:44] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=4, max_depth=2, learning_rate=0.05, booster=gbtree, base_score=0.25, score=(train=-9140.711, test=-17248.689), total=  14.7s\n",
      "[CV] n_estimators=900, min_child_weight=4, max_depth=2, learning_rate=0.05, booster=gbtree, base_score=0.25 \n",
      "[15:46:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=4, max_depth=2, learning_rate=0.05, booster=gbtree, base_score=0.25, score=(train=-8968.808, test=-17507.991), total=  14.2s\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.5 \n",
      "[15:47:13] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.5, score=(train=-831.045, test=-18249.584), total=  18.2s\n",
      "[CV] n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=0.25 \n",
      "[15:47:31] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=0.25, score=(train=-1224.359, test=-16349.456), total=   4.9s\n",
      "[CV] n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=0.25 \n",
      "[15:47:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=0.25, score=(train=-1448.616, test=-17590.535), total=   4.9s\n",
      "[CV] n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=0.25 \n",
      "[15:47:41] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=0.25, score=(train=-1635.126, test=-15435.104), total=   5.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.75 \n",
      "[CV] n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[15:48:00] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-0.143, test=-17557.702), total=  34.4s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.25 \n",
      "[15:48:35] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.25, score=(train=-21813.548, test=-21026.383), total=   1.1s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.25 \n",
      "[15:48:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.25, score=(train=-21372.364, test=-21981.951), total=   1.0s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.25 \n",
      "[15:48:37] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.25, score=(train=-21107.215, test=-25422.291), total=   1.1s\n",
      "[CV] n_estimators=900, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:48:38] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-21188.382, test=-22215.758), total=   9.6s\n",
      "[CV] n_estimators=900, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:48:48] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-20585.701, test=-24580.968), total=   9.5s\n",
      "[CV] n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=1 \n",
      "[15:48:57] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=1, score=(train=-23510.913, test=-22016.232), total=   1.1s\n",
      "[CV] n_estimators=900, min_child_weight=3, max_depth=5, learning_rate=0.05, booster=gblinear, base_score=1 \n",
      "[15:48:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=3, max_depth=5, learning_rate=0.05, booster=gblinear, base_score=1, score=(train=-21310.235, test=-22372.536), total=  10.3s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=1 \n",
      "[15:49:09] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=1, score=(train=-4624.933, test=-16036.593), total=   3.0s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=1 \n",
      "[15:49:12] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=1, score=(train=-4266.454, test=-17173.972), total=   3.0s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=1 \n",
      "[15:49:15] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=1, score=(train=-4880.477, test=-17526.016), total=   2.9s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:49:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-7.327, test=-16626.223), total= 1.7min\n",
      "[CV] n_estimators=500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25 \n",
      "[15:51:04] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25, score=(train=-21490.469, test=-22617.602), total=   5.4s\n",
      "[CV] n_estimators=500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25 \n",
      "[15:51:09] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25, score=(train=-21010.595, test=-25235.437), total=   5.4s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[15:51:14] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-21424.269, test=-25538.947), total=   1.2s\n",
      "[CV] n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:51:16] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-20989.760, test=-21255.145), total=   5.4s\n",
      "[CV] n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:51:21] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-20642.798, test=-24662.472), total=   5.3s\n",
      "[CV] n_estimators=500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.75 \n",
      "[15:51:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.75, score=(train=-7172.631, test=-16331.851), total=   8.8s\n",
      "[CV] n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25 \n",
      "[15:51:35] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25, score=(train=-23117.162, test=-22136.690), total=   1.1s\n",
      "[CV] n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25 \n",
      "[15:51:37] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25, score=(train=-22756.227, test=-24554.184), total=   1.1s\n",
      "[CV] n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25 \n",
      "[15:51:38] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25, score=(train=-22690.205, test=-23065.225), total=   1.1s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=3, learning_rate=0.05, booster=gblinear, base_score=0.5 \n",
      "[15:45:37] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=3, learning_rate=0.05, booster=gblinear, base_score=0.5, score=(train=-21010.597, test=-25235.441), total=   5.2s\n",
      "[CV] n_estimators=900, min_child_weight=1, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=1 \n",
      "[15:45:42] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=1, max_depth=15, learning_rate=0.1, booster=gbtree, base_score=1, score=(train=-7.359, test=-16001.439), total= 1.1min\n",
      "[CV] n_estimators=900, min_child_weight=4, max_depth=2, learning_rate=0.05, booster=gbtree, base_score=0.25 \n",
      "[15:46:48] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=4, max_depth=2, learning_rate=0.05, booster=gbtree, base_score=0.25, score=(train=-9338.569, test=-16238.471), total=  14.5s\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.5 \n",
      "[15:47:03] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.5, score=(train=-937.998, test=-16344.159), total=  18.1s\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.5 \n",
      "[15:47:21] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.5, score=(train=-884.432, test=-15013.018), total=  18.4s\n",
      "[CV] n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=0.25 \n",
      "[15:47:39] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=0.25, score=(train=-1660.389, test=-17573.259), total=   5.1s\n",
      "[CV] n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=0.25 \n",
      "[15:47:44] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.1, booster=gbtree, base_score=0.25, score=(train=-1838.584, test=-18268.369), total=   5.0s\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.75 \n",
      "[15:47:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.75, score=(train=-20972.338, test=-21235.545), total=  10.1s\n",
      "[CV] n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[15:48:00] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-7.332, test=-17449.472), total=  34.7s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.25 \n",
      "[15:48:35] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.25, score=(train=-21489.134, test=-22750.253), total=   1.0s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.25 \n",
      "[15:48:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.25, score=(train=-22194.092, test=-20759.835), total=   1.1s\n",
      "[CV] n_estimators=900, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:48:37] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-21366.110, test=-21219.176), total=   9.5s\n",
      "[CV] n_estimators=900, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:48:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-21863.406, test=-20873.253), total=   9.8s\n",
      "[CV] n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=1 \n",
      "[15:48:56] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=1, score=(train=-22690.215, test=-23065.233), total=   1.1s\n",
      "[CV] n_estimators=900, min_child_weight=3, max_depth=5, learning_rate=0.05, booster=gblinear, base_score=1 \n",
      "[15:48:57] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=3, max_depth=5, learning_rate=0.05, booster=gblinear, base_score=1, score=(train=-21509.056, test=-21219.786), total=  10.2s\n",
      "[CV] n_estimators=900, min_child_weight=3, max_depth=5, learning_rate=0.05, booster=gblinear, base_score=1 \n",
      "[15:49:08] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=3, max_depth=5, learning_rate=0.05, booster=gblinear, base_score=1, score=(train=-20802.496, test=-24885.498), total=   9.7s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:49:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-0.041, test=-18217.101), total= 1.8min\n",
      "[CV] n_estimators=500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25 \n",
      "[15:51:04] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25, score=(train=-21710.803, test=-21157.177), total=   5.4s\n",
      "[CV] n_estimators=500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25 \n",
      "[15:51:09] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25, score=(train=-22159.756, test=-21113.849), total=   5.4s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[15:51:14] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-22584.039, test=-21143.597), total=   1.1s\n",
      "[CV] n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:51:16] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-21187.927, test=-22204.650), total=   5.5s\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.75 \n",
      "[15:47:45] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.75, score=(train=-21366.110, test=-21219.175), total=  10.0s\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.75 \n",
      "[15:47:55] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.75, score=(train=-20585.701, test=-24580.970), total=   9.6s\n",
      "[CV] n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[15:48:04] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-7.331, test=-15452.918), total=  34.5s\n",
      "[CV] n_estimators=900, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:48:39] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-20972.337, test=-21235.545), total=   9.8s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.05, booster=gblinear, base_score=0.75 \n",
      "[15:48:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.05, booster=gblinear, base_score=0.75, score=(train=-23117.163, test=-22136.694), total=   1.1s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.05, booster=gblinear, base_score=0.75 \n",
      "[15:48:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.05, booster=gblinear, base_score=0.75, score=(train=-22756.239, test=-24554.193), total=   1.1s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.05, booster=gblinear, base_score=0.75 \n",
      "[15:48:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.05, booster=gblinear, base_score=0.75, score=(train=-22690.208, test=-23065.224), total=   1.1s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.05, booster=gblinear, base_score=0.75 \n",
      "[15:48:53] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.05, booster=gblinear, base_score=0.75, score=(train=-23510.910, test=-22016.232), total=   1.1s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.05, booster=gblinear, base_score=0.75 \n",
      "[15:48:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=2, learning_rate=0.05, booster=gblinear, base_score=0.75, score=(train=-22229.229, test=-25926.571), total=   1.1s\n",
      "[CV] n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=1 \n",
      "[15:48:55] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=1, score=(train=-23117.171, test=-22136.705), total=   1.1s\n",
      "[CV] n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=1 \n",
      "[15:48:56] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=1, score=(train=-22756.245, test=-24554.203), total=   1.1s\n",
      "[CV] n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=1 \n",
      "[15:48:57] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=1, score=(train=-22229.227, test=-25926.571), total=   1.1s\n",
      "[CV] n_estimators=900, min_child_weight=3, max_depth=5, learning_rate=0.05, booster=gblinear, base_score=1 \n",
      "[15:48:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=3, max_depth=5, learning_rate=0.05, booster=gblinear, base_score=1, score=(train=-21131.045, test=-21426.999), total=  10.4s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=1 \n",
      "[15:49:09] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=1, score=(train=-4477.984, test=-17265.720), total=   3.0s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=1 \n",
      "[15:49:12] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=1, score=(train=-4771.896, test=-14764.740), total=   2.9s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:49:15] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-7.343, test=-16428.522), total= 1.7min\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:50:59] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-0.031, test=-17892.130), total= 1.9min\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[15:52:53] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-7.331, test=-15452.918), total= 2.2min\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=10, learning_rate=0.05, booster=gblinear, base_score=0.75 \n",
      "[15:55:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=10, learning_rate=0.05, booster=gblinear, base_score=0.75, score=(train=-20672.563, test=-24682.865), total=  16.0s\n",
      "[CV] n_estimators=900, min_child_weight=3, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.25 \n",
      "[15:55:22] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=3, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.25, score=(train=-841.819, test=-17824.236), total=  17.9s\n",
      "[CV] n_estimators=1100, min_child_weight=1, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.75 \n",
      "[15:55:40] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=1, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.75, score=(train=-21364.725, test=-21220.116), total=  11.7s[15:59:24] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 250 out of 250 | elapsed: 20.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=XGBRegressor(), n_iter=50, n_jobs=4,\n",
       "                   param_distributions={'base_score': [0.25, 0.5, 0.75, 1],\n",
       "                                        'booster': ['gbtree', 'gblinear'],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
       "                                        'max_depth': [2, 3, 5, 10, 15],\n",
       "                                        'min_child_weight': [1, 2, 3, 4],\n",
       "                                        'n_estimators': [100, 500, 900, 1100,\n",
       "                                                         1500]},\n",
       "                   random_state=42, return_train_score=True,\n",
       "                   scoring='neg_mean_absolute_error', verbose=5)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, max_depth=2, n_estimators=900)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, max_depth=2, n_estimators=900)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=xgboost.XGBRegressor(base_score=0.25, max_depth=2, n_estimators=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:04:07] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, max_depth=2, n_estimators=900)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_modelV1.pkl'\n",
    "pickle.dump(classifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 174)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1958</td>\n",
       "      <td>1958</td>\n",
       "      <td>108.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>20.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 174 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          20         80.0    11622            5            6       1961   \n",
       "1          20         81.0    14267            6            6       1958   \n",
       "2          60         74.0    13830            5            5       1997   \n",
       "3          60         78.0     9978            6            6       1998   \n",
       "4         120         43.0     5005            8            5       1992   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0          1961         0.0       468.0       144.0  ...     0     0    1   \n",
       "1          1958       108.0       923.0         0.0  ...     0     0    1   \n",
       "2          1998         0.0       791.0         0.0  ...     0     0    1   \n",
       "3          1998        20.0       602.0         0.0  ...     0     0    1   \n",
       "4          1992         0.0       263.0         0.0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    0  0  \n",
       "1       1        0        0        0       0    0  0  \n",
       "2       1        0        0        0       0    0  0  \n",
       "3       1        0        0        0       0    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 174 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=regressor.predict(df_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([117275.625, 163568.39 , 188306.14 , ..., 181178.69 , 115435.21 ,\n",
       "       236526.36 ], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create Sample Submission file and Submit\n",
    "pred=pd.DataFrame(y_pred)\n",
    "sub_df=pd.read_csv('datasets/sample_submission.csv')\n",
    "datasets=pd.concat([sub_df['Id'],pred],axis=1)\n",
    "datasets.columns=['Id','SalePrice']\n",
    "datasets.to_csv('sample_submissionv1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I submitted this result to the Kaggle submission and got Rank: 1379 and Score: 0.13727. Now, I will use ANN to optimize and see if I can get better results.\n",
    "\n",
    "\n",
    "### Artificial Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.columns=['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>117275.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>163568.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188306.140625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>193885.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>185424.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>85311.835938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>66737.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>181178.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>115435.210938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>236526.359375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SalePrice\n",
       "0     117275.625000\n",
       "1     163568.390625\n",
       "2     188306.140625\n",
       "3     193885.375000\n",
       "4     185424.812500\n",
       "...             ...\n",
       "1454   85311.835938\n",
       "1455   66737.593750\n",
       "1456  181178.687500\n",
       "1457  115435.210938\n",
       "1458  236526.359375\n",
       "\n",
       "[1459 rows x 1 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25 \n",
      "[15:51:39] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25, score=(train=-22229.231, test=-25926.574), total=   1.3s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[15:51:40] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-7.324, test=-17449.416), total= 2.0min\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=1 \n",
      "[15:53:43] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=1, score=(train=-21338.039, test=-21200.299), total=  22.5s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=1 \n",
      "[15:54:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=1, score=(train=-20948.918, test=-21203.830), total=  22.5s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=1 \n",
      "[15:54:28] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=1, score=(train=-20540.705, test=-24540.215), total=  20.2s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=10, learning_rate=0.05, booster=gblinear, base_score=0.75 \n",
      "[15:54:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=10, learning_rate=0.05, booster=gblinear, base_score=0.75, score=(train=-21233.326, test=-22260.994), total=  16.7s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=10, learning_rate=0.05, booster=gblinear, base_score=0.75 \n",
      "[15:55:05] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=10, learning_rate=0.05, booster=gblinear, base_score=0.75, score=(train=-21938.178, test=-20933.720), total=  16.4s\n",
      "[CV] n_estimators=900, min_child_weight=3, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.25 \n",
      "[15:55:22] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=3, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.25, score=(train=-851.478, test=-17295.194), total=  18.0s\n",
      "[CV] n_estimators=900, min_child_weight=3, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.25 \n",
      "[15:55:40] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=3, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.25, score=(train=-944.618, test=-18301.499), total=  18.5s\n",
      "[CV] n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:55:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-21373.135, test=-21202.315), total=   5.4s\n",
      "[CV] n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:56:04] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-20989.760, test=-21255.145), total=   5.4s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:56:09] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-315.867, test=-17706.231), total=  14.4s\n",
      "[CV] n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.5 \n",
      "[15:56:24] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.5, score=(train=-13666.552, test=-18253.065), total=   1.7s\n",
      "[CV] n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.5 \n",
      "[15:56:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.5, score=(train=-13860.252, test=-18243.139), total=   1.6s\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=1 \n",
      "[15:56:27] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=1, score=(train=-20972.338, test=-21235.544), total=   9.7s\n",
      "[CV] n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gblinear, base_score=0.25 \n",
      "[15:56:37] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gblinear, base_score=0.25, score=(train=-21197.523, test=-22223.289), total=  12.4s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.5 \n",
      "[15:56:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.5, score=(train=-21419.122, test=-21240.878), total=  16.5s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.5 \n",
      "[15:57:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.5, score=(train=-20672.564, test=-24682.868), total=  15.8s\n",
      "[CV] n_estimators=1100, min_child_weight=2, max_depth=5, learning_rate=0.1, booster=gbtree, base_score=0.75 \n",
      "[15:57:22] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=2, max_depth=5, learning_rate=0.1, booster=gbtree, base_score=0.75, score=(train=-93.569, test=-14250.585), total=  31.8s\n",
      "[CV] n_estimators=1100, min_child_weight=2, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=1 \n",
      "[15:57:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=2, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=1, score=(train=-20987.290, test=-21249.680), total=  11.8s\n",
      "[CV] n_estimators=900, min_child_weight=1, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.25 \n",
      "[15:58:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=1, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.25, score=(train=-21366.111, test=-21219.174), total=   9.5s\n",
      "[CV] n_estimators=900, min_child_weight=1, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.25 \n",
      "[15:58:15] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=1, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.25, score=(train=-21863.408, test=-20873.256), total=   9.8s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=0.75 \n",
      "[15:58:25] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=0.75, score=(train=-20948.916, test=-21203.830), total=  16.6s\n",
      "[CV] n_estimators=100, min_child_weight=3, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.5 \n",
      "[15:58:42] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=3, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.5, score=(train=-21961.562, test=-20544.255), total=   1.1s\n",
      "[CV] n_estimators=1500, min_child_weight=4, max_depth=5, learning_rate=0.1, booster=gblinear, base_score=0.5 \n",
      "[15:58:43] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=4, max_depth=5, learning_rate=0.1, booster=gblinear, base_score=0.5, score=(train=-21375.608, test=-21230.667), total=  18.9s\n",
      "[CV] n_estimators=1500, min_child_weight=4, max_depth=5, learning_rate=0.1, booster=gblinear, base_score=0.5 \n",
      "[15:59:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=4, max_depth=5, learning_rate=0.1, booster=gblinear, base_score=0.5, score=(train=-20588.814, test=-24581.022), total=  16.2s\n"
     ]
    }
   ],
   "source": [
    "temp_df=df_Train['SalePrice'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[15:47:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.75, score=(train=-21188.382, test=-22215.758), total=  10.0s\n",
      "[CV] n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[15:47:56] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-7.356, test=-15952.017), total=  34.8s\n",
      "[CV] n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[15:48:31] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-0.051, test=-18295.797), total=  35.5s\n",
      "[CV] n_estimators=900, min_child_weight=3, max_depth=5, learning_rate=0.05, booster=gblinear, base_score=1 \n",
      "[15:49:07] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=3, max_depth=5, learning_rate=0.05, booster=gblinear, base_score=1, score=(train=-22021.675, test=-21005.624), total=   9.9s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:49:17] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-7.323, test=-17196.495), total= 1.8min\n",
      "[CV] n_estimators=500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25 \n",
      "[15:51:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25, score=(train=-21336.183, test=-21740.210), total=   5.4s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[15:51:11] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-22212.995, test=-21283.755), total=   1.1s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[15:51:13] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-21854.435, test=-23368.613), total=   1.1s\n",
      "[CV] n_estimators=100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[15:51:14] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-21773.859, test=-22363.166), total=   1.1s\n",
      "[CV] n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:51:15] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-21373.135, test=-21202.315), total=   5.4s\n",
      "[CV] n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:51:20] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-21883.835, test=-20855.495), total=   5.2s\n",
      "[CV] n_estimators=500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.75 \n",
      "[15:51:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.75, score=(train=-7259.402, test=-17499.803), total=   8.4s\n",
      "[CV] n_estimators=500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.75 \n",
      "[15:51:34] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.75, score=(train=-7090.311, test=-17848.721), total=   9.3s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[15:51:44] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-0.031, test=-17557.713), total= 2.0min\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=1 \n",
      "[15:53:45] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=1, score=(train=-21178.815, test=-22207.230), total=  22.4s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=1 \n",
      "[15:54:07] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=1, score=(train=-21832.902, test=-20848.437), total=  22.5s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=10, learning_rate=0.05, booster=gblinear, base_score=0.75 \n",
      "[15:54:30] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=10, learning_rate=0.05, booster=gblinear, base_score=0.75, score=(train=-21419.122, test=-21240.879), total=  19.9s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=10, learning_rate=0.05, booster=gblinear, base_score=0.75 \n",
      "[15:54:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=10, learning_rate=0.05, booster=gblinear, base_score=0.75, score=(train=-21032.895, test=-21292.673), total=  16.7s\n",
      "[CV] n_estimators=900, min_child_weight=3, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.25 \n",
      "[15:55:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=3, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.25, score=(train=-947.846, test=-16428.285), total=  18.6s\n",
      "[CV] n_estimators=900, min_child_weight=3, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.25 \n",
      "[15:55:25] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=3, max_depth=3, learning_rate=0.2, booster=gbtree, base_score=0.25, score=(train=-893.786, test=-14631.044), total=  18.1s\n",
      "[CV] n_estimators=1100, min_child_weight=1, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.75 \n",
      "[15:55:43] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=1, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.75, score=(train=-21194.278, test=-22221.814), total=  11.9s\n",
      "[CV] n_estimators=1100, min_child_weight=1, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.75 \n",
      "[15:55:55] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=1, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.75, score=(train=-20575.521, test=-24569.775), total=  11.9s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:56:07] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-331.699, test=-16123.306), total=  14.1s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:56:21] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-413.068, test=-17536.854), total=  14.3s\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=1 \n",
      "[15:56:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=1, score=(train=-20585.700, test=-24580.969), total=   9.9s\n",
      "[CV] n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gblinear, base_score=0.25 \n",
      "[15:56:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gblinear, base_score=0.25, score=(train=-21883.068, test=-20889.473), total=  12.0s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.5 \n",
      "[15:56:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.5, score=(train=-21032.894, test=-21292.676), total=  16.3s\n",
      "[CV] n_estimators=1100, min_child_weight=2, max_depth=5, learning_rate=0.1, booster=gbtree, base_score=0.75 \n",
      "[15:57:14] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=2, max_depth=5, learning_rate=0.1, booster=gbtree, base_score=0.75, score=(train=-77.076, test=-16703.699), total=  31.4s\n",
      "[CV] n_estimators=1100, min_child_weight=2, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=1 \n",
      "[15:57:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=2, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=1, score=(train=-21380.253, test=-21229.713), total=  11.3s\n",
      "[CV] n_estimators=1100, min_child_weight=2, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=1 \n",
      "[15:57:57] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=2, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=1, score=(train=-21883.070, test=-20889.476), total=  11.8s\n",
      "[CV] n_estimators=900, min_child_weight=1, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.25 \n",
      "[15:58:09] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=1, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.25, score=(train=-21188.381, test=-22215.759), total=   9.6s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=0.75 \n",
      "[15:58:19] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=0.75, score=(train=-21338.040, test=-21200.300), total=  16.3s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=0.75 \n",
      "[15:58:35] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=0.75, score=(train=-20540.704, test=-24540.214), total=  17.8s\n",
      "[CV] n_estimators=1500, min_child_weight=4, max_depth=5, learning_rate=0.1, booster=gblinear, base_score=0.5 \n",
      "[15:58:53] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=4, max_depth=5, learning_rate=0.1, booster=gblinear, base_score=0.5, score=(train=-21872.011, test=-20888.798), total=  17.5s\n",
      "[CV] n_estimators=1100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[15:59:11] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-20987.290, test=-21249.682), total=  11.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] n_estimators=500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.75 \n",
      "[15:51:21] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.75, score=(train=-7361.801, test=-15056.542), total=   8.1s\n",
      "[CV] n_estimators=500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.75 \n",
      "[15:51:29] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=3, max_depth=2, learning_rate=0.15, booster=gbtree, base_score=0.75, score=(train=-7634.430, test=-13996.257), total=   9.0s\n",
      "[CV] n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25 \n",
      "[15:51:38] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=4, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.25, score=(train=-23510.906, test=-22016.227), total=   1.2s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[15:51:40] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-7.330, test=-15951.900), total= 2.0min\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[15:53:41] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-0.033, test=-18295.795), total= 2.1min\n",
      "[CV] n_estimators=1100, min_child_weight=1, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.75 \n",
      "[15:55:47] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=1, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.75, score=(train=-20970.482, test=-21232.762), total=  11.7s\n",
      "[CV] n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:55:59] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-21187.927, test=-22204.650), total=   5.4s\n",
      "[CV] n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:56:05] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-20642.798, test=-24662.472), total=   5.3s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:56:10] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-325.096, test=-14470.127), total=  14.3s\n",
      "[CV] n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.5 \n",
      "[15:56:24] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.5, score=(train=-14009.165, test=-16760.436), total=   1.6s\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=1 \n",
      "[15:56:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=1, score=(train=-21366.110, test=-21219.177), total=   9.6s\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=1 \n",
      "[15:56:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=1, score=(train=-21863.408, test=-20873.256), total=   9.6s\n",
      "[CV] n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gblinear, base_score=0.25 \n",
      "[15:56:45] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gblinear, base_score=0.25, score=(train=-20987.290, test=-21249.681), total=  12.3s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.5 \n",
      "[15:56:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.5, score=(train=-21233.330, test=-22260.996), total=  16.4s\n",
      "[CV] n_estimators=1100, min_child_weight=2, max_depth=5, learning_rate=0.1, booster=gbtree, base_score=0.75 \n",
      "[15:57:14] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=2, max_depth=5, learning_rate=0.1, booster=gbtree, base_score=0.75, score=(train=-78.799, test=-15873.314), total=  31.5s\n",
      "[CV] n_estimators=1100, min_child_weight=2, max_depth=5, learning_rate=0.1, booster=gbtree, base_score=0.75 \n",
      "[15:57:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=2, max_depth=5, learning_rate=0.1, booster=gbtree, base_score=0.75, score=(train=-88.855, test=-16605.156), total=  31.8s\n",
      "[CV] n_estimators=900, min_child_weight=1, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.25 \n",
      "[15:58:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=1, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.25, score=(train=-20585.701, test=-24580.967), total=   9.8s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=0.75 \n",
      "[15:58:28] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=0.75, score=(train=-21832.902, test=-20848.437), total=  16.5s\n",
      "[CV] n_estimators=1500, min_child_weight=4, max_depth=5, learning_rate=0.1, booster=gblinear, base_score=0.5 \n",
      "[15:58:44] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=4, max_depth=5, learning_rate=0.1, booster=gblinear, base_score=0.5, score=(train=-20980.418, test=-21242.279), total=  19.0s\n",
      "[CV] n_estimators=1100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[15:59:03] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-21380.253, test=-21229.713), total=  12.1s\n",
      "[CV] n_estimators=1100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[15:59:15] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-20609.898, test=-24605.909), total=   8.6s\n",
      "\n",
      "[CV] n_estimators=1100, min_child_weight=1, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.75 \n",
      "[15:55:52] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=1, max_depth=2, learning_rate=0.15, booster=gblinear, base_score=0.75, score=(train=-21859.221, test=-20874.484), total=  12.0s\n",
      "[CV] n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[15:56:04] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=3, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-21883.835, test=-20855.495), total=   5.4s\n",
      "[CV] n_estimators=500, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=0.25 \n",
      "[15:56:10] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=500, min_child_weight=4, max_depth=5, learning_rate=0.15, booster=gbtree, base_score=0.25, score=(train=-370.775, test=-17294.816), total=  14.2s\n",
      "[CV] n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.5 \n",
      "[15:56:24] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.5, score=(train=-13793.530, test=-16662.293), total=   1.6s\n",
      "[CV] n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.5 \n",
      "[15:56:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.5, score=(train=-14574.535, test=-16672.266), total=   1.6s\n",
      "[CV] n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=1 \n",
      "[15:56:27] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=2, max_depth=3, learning_rate=0.15, booster=gblinear, base_score=1, score=(train=-21188.385, test=-22215.763), total=   9.6s\n",
      "[CV] n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gblinear, base_score=0.25 \n",
      "[15:56:37] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gblinear, base_score=0.25, score=(train=-21380.250, test=-21229.712), total=  12.3s\n",
      "[CV] n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gblinear, base_score=0.25 \n",
      "[15:56:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=4, max_depth=3, learning_rate=0.1, booster=gblinear, base_score=0.25, score=(train=-20609.896, test=-24605.907), total=  12.0s\n",
      "[CV] n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.5 \n",
      "[15:57:01] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=2, max_depth=15, learning_rate=0.05, booster=gblinear, base_score=0.5, score=(train=-21938.178, test=-20933.720), total=  16.4s\n",
      "[CV] n_estimators=1100, min_child_weight=2, max_depth=5, learning_rate=0.1, booster=gbtree, base_score=0.75 \n",
      "[15:57:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=2, max_depth=5, learning_rate=0.1, booster=gbtree, base_score=0.75, score=(train=-71.028, test=-16995.531), total=  31.6s\n",
      "[CV] n_estimators=1100, min_child_weight=2, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=1 \n",
      "[15:57:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=2, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=1, score=(train=-21197.523, test=-22223.287), total=  11.8s\n",
      "[CV] n_estimators=1100, min_child_weight=2, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=1 \n",
      "[15:58:01] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=2, max_depth=10, learning_rate=0.1, booster=gblinear, base_score=1, score=(train=-20609.897, test=-24605.907), total=  11.8s\n",
      "[CV] n_estimators=900, min_child_weight=1, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.25 \n",
      "[15:58:13] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=900, min_child_weight=1, max_depth=5, learning_rate=0.15, booster=gblinear, base_score=0.25, score=(train=-20972.336, test=-21235.544), total=   9.8s\n",
      "[CV] n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=0.75 \n",
      "[15:58:23] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=3, max_depth=2, learning_rate=0.2, booster=gblinear, base_score=0.75, score=(train=-21178.815, test=-22207.231), total=  16.4s\n",
      "[CV] n_estimators=100, min_child_weight=3, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.5 \n",
      "[15:58:39] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=3, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.5, score=(train=-21572.418, test=-20916.685), total=   1.1s\n",
      "[CV] n_estimators=100, min_child_weight=3, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.5 \n",
      "[15:58:41] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=3, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.5, score=(train=-21285.076, test=-22436.106), total=   1.1s\n",
      "[CV] n_estimators=100, min_child_weight=3, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.5 \n",
      "[15:58:42] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=3, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.5, score=(train=-21141.301, test=-21741.792), total=   1.1s\n",
      "[CV] n_estimators=100, min_child_weight=3, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.5 \n",
      "[15:58:43] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=100, min_child_weight=3, max_depth=15, learning_rate=0.2, booster=gblinear, base_score=0.5, score=(train=-20944.743, test=-25323.212), total=   1.2s\n",
      "[CV] n_estimators=1500, min_child_weight=4, max_depth=5, learning_rate=0.1, booster=gblinear, base_score=0.5 \n",
      "[15:58:44] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1500, min_child_weight=4, max_depth=5, learning_rate=0.1, booster=gblinear, base_score=0.5, score=(train=-21201.570, test=-22230.062), total=  19.0s\n",
      "[CV] n_estimators=1100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[15:59:03] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-21197.523, test=-22223.286), total=  11.9s\n",
      "[CV] n_estimators=1100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[15:59:15] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimators=1100, min_child_weight=2, max_depth=15, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-21883.068, test=-20889.475), total=   8.8s\n"
     ]
    }
   ],
   "source": [
    "temp_df.column=['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500.0\n",
       "1       181500.0\n",
       "2       223500.0\n",
       "3       140000.0\n",
       "4       250000.0\n",
       "          ...   \n",
       "1455    175000.0\n",
       "1456    210000.0\n",
       "1457    266500.0\n",
       "1458    142125.0\n",
       "1459    147500.0\n",
       "Name: SalePrice, Length: 1422, dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snigdhakakkar/opt/anaconda2/envs/tf/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "df_Train.drop(['SalePrice'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=pd.concat([df_Train,temp_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>208500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>181500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>223500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>250000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          60         65.0     8450            7            5       2003   \n",
       "1          20         80.0     9600            6            8       1976   \n",
       "2          60         68.0    11250            7            5       2001   \n",
       "3          70         60.0     9550            7            5       1915   \n",
       "4          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min2  Typ  Attchd  \\\n",
       "0          2003       196.0       706.0         0.0  ...     0    1       1   \n",
       "1          1976         0.0       978.0         0.0  ...     0    1       1   \n",
       "2          2002       162.0       486.0         0.0  ...     0    1       1   \n",
       "3          1970         0.0       216.0         0.0  ...     0    1       0   \n",
       "4          2000       350.0       655.0         0.0  ...     0    1       1   \n",
       "\n",
       "   Basment  BuiltIn  CarPort  Detchd  RFn  P  SalePrice  \n",
       "0        0        0        0       0    1  0   208500.0  \n",
       "1        0        0        0       0    1  0   181500.0  \n",
       "2        0        0        0       0    1  0   223500.0  \n",
       "3        0        0        0       1    0  0   140000.0  \n",
       "4        0        0        0       0    1  0   250000.0  \n",
       "\n",
       "[5 rows x 175 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test=pd.concat([df_Test,pred],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=pd.concat([df_Train,df_Test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 175)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df_Train.drop(['SalePrice'],axis=1)\n",
    "y_train=df_Train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2881, 174), (2881,))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing ANN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU,PReLU,ELU\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 2304 samples, validate on 577 samples\n",
      "Epoch 1/1000\n",
      "2304/2304 [==============================] - 3s 2ms/sample - loss: 100610.3061 - val_loss: 53920.3239\n",
      "Epoch 2/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 62624.8602 - val_loss: 48092.0935\n",
      "Epoch 3/1000\n",
      "2304/2304 [==============================] - 1s 491us/sample - loss: 53974.8049 - val_loss: 40582.9772\n",
      "Epoch 4/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 46423.7883 - val_loss: 39023.5726\n",
      "Epoch 5/1000\n",
      "2304/2304 [==============================] - 1s 505us/sample - loss: 40460.0472 - val_loss: 33138.0150\n",
      "Epoch 6/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 37726.4488 - val_loss: 32836.1822\n",
      "Epoch 7/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 36518.4189 - val_loss: 33322.0319\n",
      "Epoch 8/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 35541.1317 - val_loss: 32292.1495\n",
      "Epoch 9/1000\n",
      "2304/2304 [==============================] - 1s 507us/sample - loss: 35461.9649 - val_loss: 32268.5393\n",
      "Epoch 10/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 34947.9896 - val_loss: 31652.3054\n",
      "Epoch 11/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 34822.8525 - val_loss: 30655.1447\n",
      "Epoch 12/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 34096.2254 - val_loss: 31537.0308\n",
      "Epoch 13/1000\n",
      "2304/2304 [==============================] - 1s 554us/sample - loss: 33974.1918 - val_loss: 30167.2239\n",
      "Epoch 14/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 33917.8122 - val_loss: 30334.6633\n",
      "Epoch 15/1000\n",
      "2304/2304 [==============================] - 1s 568us/sample - loss: 33680.8146 - val_loss: 31432.3647\n",
      "Epoch 16/1000\n",
      "2304/2304 [==============================] - 1s 543us/sample - loss: 33635.4792 - val_loss: 29925.0898\n",
      "Epoch 17/1000\n",
      "2304/2304 [==============================] - 1s 526us/sample - loss: 33418.1206 - val_loss: 30590.0053\n",
      "Epoch 18/1000\n",
      "2304/2304 [==============================] - 1s 545us/sample - loss: 33419.2393 - val_loss: 30393.9837\n",
      "Epoch 19/1000\n",
      "2304/2304 [==============================] - 1s 559us/sample - loss: 33513.8641 - val_loss: 30236.4205\n",
      "Epoch 20/1000\n",
      "2304/2304 [==============================] - 1s 615us/sample - loss: 33234.8799 - val_loss: 30102.8052\n",
      "Epoch 21/1000\n",
      "2304/2304 [==============================] - 1s 581us/sample - loss: 33246.6701 - val_loss: 29429.4367\n",
      "Epoch 22/1000\n",
      "2304/2304 [==============================] - 1s 521us/sample - loss: 32860.5211 - val_loss: 29957.9248\n",
      "Epoch 23/1000\n",
      "2304/2304 [==============================] - 1s 535us/sample - loss: 32883.4507 - val_loss: 29349.3471\n",
      "Epoch 24/1000\n",
      "2304/2304 [==============================] - 2s 657us/sample - loss: 32913.3759 - val_loss: 29353.6534\n",
      "Epoch 25/1000\n",
      "2304/2304 [==============================] - 2s 663us/sample - loss: 32452.2175 - val_loss: 29788.0761\n",
      "Epoch 26/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 32687.9497 - val_loss: 29016.0328\n",
      "Epoch 27/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 32222.0788 - val_loss: 28936.3364\n",
      "Epoch 28/1000\n",
      "2304/2304 [==============================] - 1s 517us/sample - loss: 32261.1786 - val_loss: 28674.5366\n",
      "Epoch 29/1000\n",
      "2304/2304 [==============================] - 1s 615us/sample - loss: 32036.1419 - val_loss: 30307.3375\n",
      "Epoch 30/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 31960.6756 - val_loss: 30112.2263\n",
      "Epoch 31/1000\n",
      "2304/2304 [==============================] - 1s 454us/sample - loss: 31632.5484 - val_loss: 28935.3872\n",
      "Epoch 32/1000\n",
      "2304/2304 [==============================] - 1s 451us/sample - loss: 31830.0193 - val_loss: 30176.7486\n",
      "Epoch 33/1000\n",
      "2304/2304 [==============================] - 1s 454us/sample - loss: 31831.9585 - val_loss: 29641.6529\n",
      "Epoch 34/1000\n",
      "2304/2304 [==============================] - 1s 462us/sample - loss: 31508.0869 - val_loss: 28997.1011\n",
      "Epoch 35/1000\n",
      "2304/2304 [==============================] - 1s 457us/sample - loss: 31326.9018 - val_loss: 28319.3762\n",
      "Epoch 36/1000\n",
      "2304/2304 [==============================] - 1s 468us/sample - loss: 31750.5470 - val_loss: 28333.5249\n",
      "Epoch 37/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 31624.1405 - val_loss: 28495.3501\n",
      "Epoch 38/1000\n",
      "2304/2304 [==============================] - 2s 810us/sample - loss: 31593.8305 - val_loss: 28315.0786\n",
      "Epoch 39/1000\n",
      "2304/2304 [==============================] - 1s 622us/sample - loss: 31036.4955 - val_loss: 28765.5946\n",
      "Epoch 40/1000\n",
      "2304/2304 [==============================] - 1s 591us/sample - loss: 31669.8403 - val_loss: 28264.0098\n",
      "Epoch 41/1000\n",
      "2304/2304 [==============================] - 1s 634us/sample - loss: 31338.6265 - val_loss: 28342.5456\n",
      "Epoch 42/1000\n",
      "2304/2304 [==============================] - 1s 529us/sample - loss: 31303.5840 - val_loss: 28900.1821\n",
      "Epoch 43/1000\n",
      "2304/2304 [==============================] - 1s 528us/sample - loss: 30870.7887 - val_loss: 28138.8309\n",
      "Epoch 44/1000\n",
      "2304/2304 [==============================] - 1s 529us/sample - loss: 30695.8531 - val_loss: 28137.4394\n",
      "Epoch 45/1000\n",
      "2304/2304 [==============================] - 1s 526us/sample - loss: 30881.6718 - val_loss: 28153.5479\n",
      "Epoch 46/1000\n",
      "2304/2304 [==============================] - 1s 584us/sample - loss: 31015.2576 - val_loss: 27978.9441\n",
      "Epoch 47/1000\n",
      "2304/2304 [==============================] - 1s 579us/sample - loss: 30487.0029 - val_loss: 28154.7525\n",
      "Epoch 48/1000\n",
      "2304/2304 [==============================] - 1s 557us/sample - loss: 30583.6706 - val_loss: 27917.8839\n",
      "Epoch 49/1000\n",
      "2304/2304 [==============================] - 1s 525us/sample - loss: 30673.0209 - val_loss: 29564.2071\n",
      "Epoch 50/1000\n",
      "2304/2304 [==============================] - 1s 524us/sample - loss: 30910.1915 - val_loss: 27769.0683\n",
      "Epoch 51/1000\n",
      "2304/2304 [==============================] - 1s 529us/sample - loss: 30522.2922 - val_loss: 27664.8555\n",
      "Epoch 52/1000\n",
      "2304/2304 [==============================] - 1s 560us/sample - loss: 30587.3716 - val_loss: 27560.1852\n",
      "Epoch 53/1000\n",
      "2304/2304 [==============================] - 1s 606us/sample - loss: 30435.6309 - val_loss: 27583.7789\n",
      "Epoch 54/1000\n",
      "2304/2304 [==============================] - 1s 553us/sample - loss: 30724.4877 - val_loss: 27460.4943\n",
      "Epoch 55/1000\n",
      "2304/2304 [==============================] - 1s 510us/sample - loss: 30538.4157 - val_loss: 28276.1866\n",
      "Epoch 56/1000\n",
      "2304/2304 [==============================] - 1s 575us/sample - loss: 30360.6723 - val_loss: 27711.3692\n",
      "Epoch 57/1000\n",
      "2304/2304 [==============================] - 1s 541us/sample - loss: 30395.0303 - val_loss: 27353.6196\n",
      "Epoch 58/1000\n",
      "2304/2304 [==============================] - 1s 606us/sample - loss: 30176.7281 - val_loss: 27270.7864\n",
      "Epoch 59/1000\n",
      "2304/2304 [==============================] - 1s 621us/sample - loss: 30203.6881 - val_loss: 27713.6288\n",
      "Epoch 60/1000\n",
      "2304/2304 [==============================] - 1s 542us/sample - loss: 30702.7377 - val_loss: 27562.7584\n",
      "Epoch 61/1000\n",
      "2304/2304 [==============================] - 1s 548us/sample - loss: 30108.6832 - val_loss: 28716.9501\n",
      "Epoch 62/1000\n",
      "2304/2304 [==============================] - 1s 515us/sample - loss: 30156.1956 - val_loss: 27368.4642\n",
      "Epoch 63/1000\n",
      "2304/2304 [==============================] - 1s 592us/sample - loss: 30081.7587 - val_loss: 27119.9706\n",
      "Epoch 64/1000\n",
      "2304/2304 [==============================] - 1s 577us/sample - loss: 29651.3897 - val_loss: 27148.4991\n",
      "Epoch 65/1000\n",
      "2304/2304 [==============================] - 1s 600us/sample - loss: 29876.7276 - val_loss: 27070.8288\n",
      "Epoch 66/1000\n",
      "2304/2304 [==============================] - 1s 587us/sample - loss: 29659.3435 - val_loss: 26871.1599\n",
      "Epoch 67/1000\n",
      "2304/2304 [==============================] - 1s 547us/sample - loss: 29848.2732 - val_loss: 26667.4163\n",
      "Epoch 68/1000\n",
      "2304/2304 [==============================] - 2s 704us/sample - loss: 29911.6975 - val_loss: 27942.2733\n",
      "Epoch 69/1000\n",
      "2304/2304 [==============================] - 1s 529us/sample - loss: 29704.0933 - val_loss: 31413.2878\n",
      "Epoch 70/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 1s 497us/sample - loss: 29692.0236 - val_loss: 29875.6174\n",
      "Epoch 71/1000\n",
      "2304/2304 [==============================] - 1s 509us/sample - loss: 29444.5205 - val_loss: 26540.1107\n",
      "Epoch 72/1000\n",
      "2304/2304 [==============================] - 1s 589us/sample - loss: 29459.9074 - val_loss: 26313.4876\n",
      "Epoch 73/1000\n",
      "2304/2304 [==============================] - 1s 547us/sample - loss: 29191.8262 - val_loss: 29106.9236\n",
      "Epoch 74/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 29503.3013 - val_loss: 26534.2318\n",
      "Epoch 75/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 29280.2554 - val_loss: 27114.8462\n",
      "Epoch 76/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 29174.5754 - val_loss: 26228.2049\n",
      "Epoch 77/1000\n",
      "2304/2304 [==============================] - 1s 511us/sample - loss: 29214.9228 - val_loss: 26112.4929\n",
      "Epoch 78/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 28873.0128 - val_loss: 26187.2956\n",
      "Epoch 79/1000\n",
      "2304/2304 [==============================] - 1s 556us/sample - loss: 29076.6146 - val_loss: 26246.6634\n",
      "Epoch 80/1000\n",
      "2304/2304 [==============================] - 1s 547us/sample - loss: 28926.0383 - val_loss: 26008.6826\n",
      "Epoch 81/1000\n",
      "2304/2304 [==============================] - 1s 530us/sample - loss: 29178.6569 - val_loss: 25950.4734\n",
      "Epoch 82/1000\n",
      "2304/2304 [==============================] - 1s 531us/sample - loss: 28794.4688 - val_loss: 25802.7251\n",
      "Epoch 83/1000\n",
      "2304/2304 [==============================] - 1s 516us/sample - loss: 28497.4521 - val_loss: 25776.8561\n",
      "Epoch 84/1000\n",
      "2304/2304 [==============================] - 1s 546us/sample - loss: 29209.5125 - val_loss: 26481.1074\n",
      "Epoch 85/1000\n",
      "2304/2304 [==============================] - 1s 547us/sample - loss: 28836.8736 - val_loss: 25371.7229\n",
      "Epoch 86/1000\n",
      "2304/2304 [==============================] - 1s 529us/sample - loss: 28778.2517 - val_loss: 25462.6746\n",
      "Epoch 87/1000\n",
      "2304/2304 [==============================] - 1s 520us/sample - loss: 28882.7681 - val_loss: 25682.1179\n",
      "Epoch 88/1000\n",
      "2304/2304 [==============================] - 1s 539us/sample - loss: 28837.9954 - val_loss: 25500.8541\n",
      "Epoch 89/1000\n",
      "2304/2304 [==============================] - 1s 568us/sample - loss: 28510.6113 - val_loss: 24963.1985\n",
      "Epoch 90/1000\n",
      "2304/2304 [==============================] - 1s 536us/sample - loss: 28874.6435 - val_loss: 25083.2892\n",
      "Epoch 91/1000\n",
      "2304/2304 [==============================] - 1s 531us/sample - loss: 28196.7937 - val_loss: 25022.3420\n",
      "Epoch 92/1000\n",
      "2304/2304 [==============================] - 1s 535us/sample - loss: 28186.3717 - val_loss: 25626.9715\n",
      "Epoch 93/1000\n",
      "2304/2304 [==============================] - 1s 546us/sample - loss: 28315.5503 - val_loss: 24764.5566\n",
      "Epoch 94/1000\n",
      "2304/2304 [==============================] - 1s 554us/sample - loss: 28277.4513 - val_loss: 26074.8776\n",
      "Epoch 95/1000\n",
      "2304/2304 [==============================] - 1s 528us/sample - loss: 28290.9967 - val_loss: 24320.6386\n",
      "Epoch 96/1000\n",
      "2304/2304 [==============================] - 1s 536us/sample - loss: 28168.5414 - val_loss: 25069.6181\n",
      "Epoch 97/1000\n",
      "2304/2304 [==============================] - 1s 554us/sample - loss: 28780.3138 - val_loss: 24540.4525\n",
      "Epoch 98/1000\n",
      "2304/2304 [==============================] - 1s 561us/sample - loss: 27641.9019 - val_loss: 24361.5829\n",
      "Epoch 99/1000\n",
      "2304/2304 [==============================] - 1s 581us/sample - loss: 28365.5158 - val_loss: 24478.9880\n",
      "Epoch 100/1000\n",
      "2304/2304 [==============================] - 1s 537us/sample - loss: 27391.5248 - val_loss: 24523.1964\n",
      "Epoch 101/1000\n",
      "2304/2304 [==============================] - 1s 550us/sample - loss: 27555.1245 - val_loss: 24297.4664\n",
      "Epoch 102/1000\n",
      "2304/2304 [==============================] - 1s 527us/sample - loss: 28051.0089 - val_loss: 24259.4297\n",
      "Epoch 103/1000\n",
      "2304/2304 [==============================] - 1s 606us/sample - loss: 27378.0516 - val_loss: 23708.0773\n",
      "Epoch 104/1000\n",
      "2304/2304 [==============================] - 1s 531us/sample - loss: 27690.5622 - val_loss: 23560.4100\n",
      "Epoch 105/1000\n",
      "2304/2304 [==============================] - 1s 521us/sample - loss: 27422.6116 - val_loss: 24302.5044\n",
      "Epoch 106/1000\n",
      "2304/2304 [==============================] - 1s 523us/sample - loss: 27231.6267 - val_loss: 23768.0313\n",
      "Epoch 107/1000\n",
      "2304/2304 [==============================] - 1s 526us/sample - loss: 27515.2533 - val_loss: 23522.7302\n",
      "Epoch 108/1000\n",
      "2304/2304 [==============================] - 1s 514us/sample - loss: 27150.2192 - val_loss: 23543.7445\n",
      "Epoch 109/1000\n",
      "2304/2304 [==============================] - 1s 616us/sample - loss: 27356.3775 - val_loss: 23211.5403\n",
      "Epoch 110/1000\n",
      "2304/2304 [==============================] - 1s 528us/sample - loss: 27045.6239 - val_loss: 23331.8817\n",
      "Epoch 111/1000\n",
      "2304/2304 [==============================] - 1s 511us/sample - loss: 27112.7747 - val_loss: 24981.8195\n",
      "Epoch 112/1000\n",
      "2304/2304 [==============================] - 1s 571us/sample - loss: 27209.0865 - val_loss: 23055.4057\n",
      "Epoch 113/1000\n",
      "2304/2304 [==============================] - 1s 527us/sample - loss: 26727.4786 - val_loss: 22777.1433\n",
      "Epoch 114/1000\n",
      "2304/2304 [==============================] - 1s 534us/sample - loss: 26954.9113 - val_loss: 23334.6140\n",
      "Epoch 115/1000\n",
      "2304/2304 [==============================] - 1s 536us/sample - loss: 26746.1559 - val_loss: 23025.9047\n",
      "Epoch 116/1000\n",
      "2304/2304 [==============================] - 1s 587us/sample - loss: 26014.0828 - val_loss: 24368.5585\n",
      "Epoch 117/1000\n",
      "2304/2304 [==============================] - 1s 538us/sample - loss: 26060.2748 - val_loss: 22636.1909\n",
      "Epoch 118/1000\n",
      "2304/2304 [==============================] - 2s 655us/sample - loss: 26247.6646 - val_loss: 22738.4760\n",
      "Epoch 119/1000\n",
      "2304/2304 [==============================] - 1s 531us/sample - loss: 26326.5711 - val_loss: 25863.9131\n",
      "Epoch 120/1000\n",
      "2304/2304 [==============================] - 1s 540us/sample - loss: 26862.1955 - val_loss: 22053.2185\n",
      "Epoch 121/1000\n",
      "2304/2304 [==============================] - 1s 545us/sample - loss: 26157.4632 - val_loss: 22017.7117\n",
      "Epoch 122/1000\n",
      "2304/2304 [==============================] - 1s 538us/sample - loss: 26178.6687 - val_loss: 22440.6422\n",
      "Epoch 123/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 26361.7464 - val_loss: 21621.3127\n",
      "Epoch 124/1000\n",
      "2304/2304 [==============================] - 1s 505us/sample - loss: 26015.4047 - val_loss: 22588.3042\n",
      "Epoch 125/1000\n",
      "2304/2304 [==============================] - 1s 510us/sample - loss: 25611.4701 - val_loss: 22587.7146\n",
      "Epoch 126/1000\n",
      "2304/2304 [==============================] - 1s 514us/sample - loss: 25849.5396 - val_loss: 22703.1635\n",
      "Epoch 127/1000\n",
      "2304/2304 [==============================] - 1s 528us/sample - loss: 25920.6568 - val_loss: 21364.6176\n",
      "Epoch 128/1000\n",
      "2304/2304 [==============================] - 1s 521us/sample - loss: 26062.8038 - val_loss: 25953.9406\n",
      "Epoch 129/1000\n",
      "2304/2304 [==============================] - 1s 521us/sample - loss: 25480.8741 - val_loss: 21225.0446\n",
      "Epoch 130/1000\n",
      "2304/2304 [==============================] - 1s 527us/sample - loss: 25521.7283 - val_loss: 21418.7970\n",
      "Epoch 131/1000\n",
      "2304/2304 [==============================] - 1s 526us/sample - loss: 25497.2117 - val_loss: 20749.5133\n",
      "Epoch 132/1000\n",
      "2304/2304 [==============================] - 1s 516us/sample - loss: 25523.5388 - val_loss: 21052.7047\n",
      "Epoch 133/1000\n",
      "2304/2304 [==============================] - 1s 530us/sample - loss: 25437.4611 - val_loss: 20763.1114\n",
      "Epoch 134/1000\n",
      "2304/2304 [==============================] - 1s 507us/sample - loss: 25175.2429 - val_loss: 20472.9195\n",
      "Epoch 135/1000\n",
      "2304/2304 [==============================] - 1s 510us/sample - loss: 25271.6833 - val_loss: 21109.6581\n",
      "Epoch 136/1000\n",
      "2304/2304 [==============================] - 1s 507us/sample - loss: 25821.0229 - val_loss: 21808.5815\n",
      "Epoch 137/1000\n",
      "2304/2304 [==============================] - 1s 513us/sample - loss: 24927.8963 - val_loss: 22013.6297\n",
      "Epoch 138/1000\n",
      "2304/2304 [==============================] - 1s 520us/sample - loss: 25071.5301 - val_loss: 20168.8326\n",
      "Epoch 139/1000\n",
      "2304/2304 [==============================] - 1s 534us/sample - loss: 24843.7774 - val_loss: 20521.0940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 24500.3124 - val_loss: 20853.1173\n",
      "Epoch 141/1000\n",
      "2304/2304 [==============================] - 1s 491us/sample - loss: 25239.1302 - val_loss: 22036.4961\n",
      "Epoch 142/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 24234.8707 - val_loss: 21574.0851\n",
      "Epoch 143/1000\n",
      "2304/2304 [==============================] - 1s 519us/sample - loss: 25061.6298 - val_loss: 20099.0046\n",
      "Epoch 144/1000\n",
      "2304/2304 [==============================] - 1s 482us/sample - loss: 24424.9266 - val_loss: 27106.5652\n",
      "Epoch 145/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 24742.8442 - val_loss: 26709.8216\n",
      "Epoch 146/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 24833.5745 - val_loss: 19753.6097\n",
      "Epoch 147/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 24647.3930 - val_loss: 23213.5274\n",
      "Epoch 148/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 24155.4408 - val_loss: 19296.6317\n",
      "Epoch 149/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 24487.7532 - val_loss: 22535.1039\n",
      "Epoch 150/1000\n",
      "2304/2304 [==============================] - 1s 491us/sample - loss: 24430.9374 - val_loss: 22041.7285\n",
      "Epoch 151/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 24450.0567 - val_loss: 24254.3441\n",
      "Epoch 152/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 24531.0926 - val_loss: 21681.2905\n",
      "Epoch 153/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 24575.0996 - val_loss: 19871.7048\n",
      "Epoch 154/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 23812.6586 - val_loss: 19402.2748\n",
      "Epoch 155/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 24343.5288 - val_loss: 20702.2949\n",
      "Epoch 156/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 24261.7196 - val_loss: 18901.2124\n",
      "Epoch 157/1000\n",
      "2304/2304 [==============================] - 1s 522us/sample - loss: 23839.8697 - val_loss: 18723.8676\n",
      "Epoch 158/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 23811.4002 - val_loss: 19549.6554\n",
      "Epoch 159/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 24574.1991 - val_loss: 20127.5736\n",
      "Epoch 160/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 23236.2830 - val_loss: 18662.3466\n",
      "Epoch 161/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 23701.2932 - val_loss: 20791.2513\n",
      "Epoch 162/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 23365.4585 - val_loss: 18931.0338\n",
      "Epoch 163/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 23588.8442 - val_loss: 21048.4823\n",
      "Epoch 164/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 23140.9264 - val_loss: 21709.2930\n",
      "Epoch 165/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 23352.2205 - val_loss: 20472.7356\n",
      "Epoch 166/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 24088.2158 - val_loss: 18257.2987\n",
      "Epoch 167/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 24237.2660 - val_loss: 18462.2225\n",
      "Epoch 168/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 23327.6688 - val_loss: 20416.6125\n",
      "Epoch 169/1000\n",
      "2304/2304 [==============================] - 1s 534us/sample - loss: 24254.4452 - val_loss: 18178.3579\n",
      "Epoch 170/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 23533.2989 - val_loss: 18793.5340\n",
      "Epoch 171/1000\n",
      "2304/2304 [==============================] - 1s 519us/sample - loss: 23409.6189 - val_loss: 17545.7183\n",
      "Epoch 172/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 22976.9139 - val_loss: 24452.5880\n",
      "Epoch 173/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 23725.1527 - val_loss: 18951.0006\n",
      "Epoch 174/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 22914.9719 - val_loss: 19052.6949\n",
      "Epoch 175/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 23604.0365 - val_loss: 18405.9645\n",
      "Epoch 176/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 23119.2740 - val_loss: 19371.3947\n",
      "Epoch 177/1000\n",
      "2304/2304 [==============================] - 1s 491us/sample - loss: 22757.7514 - val_loss: 18145.1736\n",
      "Epoch 178/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 23084.6837 - val_loss: 20430.9641\n",
      "Epoch 179/1000\n",
      "2304/2304 [==============================] - 1s 510us/sample - loss: 22323.6842 - val_loss: 19503.8446\n",
      "Epoch 180/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 22704.4871 - val_loss: 18164.7841\n",
      "Epoch 181/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 22800.5813 - val_loss: 19625.4375\n",
      "Epoch 182/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 22404.5147 - val_loss: 18798.1932\n",
      "Epoch 183/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 22631.2683 - val_loss: 20919.1315\n",
      "Epoch 184/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 23109.0785 - val_loss: 18250.0096\n",
      "Epoch 185/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 22687.4152 - val_loss: 17392.8263\n",
      "Epoch 186/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 22429.9722 - val_loss: 22048.2026\n",
      "Epoch 187/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 22563.2281 - val_loss: 17541.0315\n",
      "Epoch 188/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 22574.8428 - val_loss: 17938.2447\n",
      "Epoch 189/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 22140.8838 - val_loss: 20991.9419\n",
      "Epoch 190/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 22150.7675 - val_loss: 18645.1129\n",
      "Epoch 191/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 22857.7176 - val_loss: 17217.1778\n",
      "Epoch 192/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 22177.2461 - val_loss: 21641.2229\n",
      "Epoch 193/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 22504.5928 - val_loss: 17777.0656\n",
      "Epoch 194/1000\n",
      "2304/2304 [==============================] - 1s 521us/sample - loss: 22249.9446 - val_loss: 18645.4987\n",
      "Epoch 195/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 23609.4071 - val_loss: 17033.7016\n",
      "Epoch 196/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 22014.1527 - val_loss: 23615.6249\n",
      "Epoch 197/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 22607.2594 - val_loss: 17630.6239\n",
      "Epoch 198/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 22185.4915 - val_loss: 21829.8239\n",
      "Epoch 199/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 22565.0476 - val_loss: 18986.3101\n",
      "Epoch 200/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 22241.6388 - val_loss: 17923.9048\n",
      "Epoch 201/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 21769.1192 - val_loss: 17170.1656\n",
      "Epoch 202/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 22830.5766 - val_loss: 19603.3910\n",
      "Epoch 203/1000\n",
      "2304/2304 [==============================] - 1s 568us/sample - loss: 22303.4083 - val_loss: 16860.2936\n",
      "Epoch 204/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 21323.9237 - val_loss: 17286.9362\n",
      "Epoch 205/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 22352.9225 - val_loss: 21780.4596\n",
      "Epoch 206/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 21904.6968 - val_loss: 19587.9213\n",
      "Epoch 207/1000\n",
      "2304/2304 [==============================] - 1s 525us/sample - loss: 22279.7191 - val_loss: 16656.2480\n",
      "Epoch 208/1000\n",
      "2304/2304 [==============================] - 1s 509us/sample - loss: 22064.9624 - val_loss: 17374.7552\n",
      "Epoch 209/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 1s 485us/sample - loss: 21641.1079 - val_loss: 17902.7225\n",
      "Epoch 210/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 21787.8379 - val_loss: 17926.2821\n",
      "Epoch 211/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 21799.7741 - val_loss: 17566.6574\n",
      "Epoch 212/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 21564.2629 - val_loss: 20316.5893\n",
      "Epoch 213/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 22114.7999 - val_loss: 19941.8110\n",
      "Epoch 214/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 21901.6676 - val_loss: 16867.5774\n",
      "Epoch 215/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 21368.6891 - val_loss: 17208.5561\n",
      "Epoch 216/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 21540.4161 - val_loss: 16543.0084\n",
      "Epoch 217/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 21834.0014 - val_loss: 16197.3770\n",
      "Epoch 218/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 21233.8662 - val_loss: 19413.4970\n",
      "Epoch 219/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 21341.7204 - val_loss: 16292.5579\n",
      "Epoch 220/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 21092.4978 - val_loss: 18258.6386\n",
      "Epoch 221/1000\n",
      "2304/2304 [==============================] - 1s 568us/sample - loss: 21409.2454 - val_loss: 17323.3259\n",
      "Epoch 222/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 21422.1761 - val_loss: 21723.1776\n",
      "Epoch 223/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 21238.9494 - val_loss: 16310.5752\n",
      "Epoch 224/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 21533.5528 - val_loss: 16409.0277\n",
      "Epoch 225/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 21737.6322 - val_loss: 17993.0428\n",
      "Epoch 226/1000\n",
      "2304/2304 [==============================] - 1s 483us/sample - loss: 21060.5876 - val_loss: 20687.0664\n",
      "Epoch 227/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 22299.8009 - val_loss: 16965.2150\n",
      "Epoch 228/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 20883.7826 - val_loss: 16588.2785\n",
      "Epoch 229/1000\n",
      "2304/2304 [==============================] - 1s 519us/sample - loss: 21222.5615 - val_loss: 16848.6609\n",
      "Epoch 230/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 20677.9760 - val_loss: 17880.4860\n",
      "Epoch 231/1000\n",
      "2304/2304 [==============================] - 1s 482us/sample - loss: 21718.9224 - val_loss: 16975.7215\n",
      "Epoch 232/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 21043.0735 - val_loss: 17924.4253\n",
      "Epoch 233/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 21550.9871 - val_loss: 16996.3131\n",
      "Epoch 234/1000\n",
      "2304/2304 [==============================] - 1s 505us/sample - loss: 21112.1802 - val_loss: 19250.4652\n",
      "Epoch 235/1000\n",
      "2304/2304 [==============================] - 1s 491us/sample - loss: 21236.5271 - val_loss: 15970.8808\n",
      "Epoch 236/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 20799.6546 - val_loss: 16421.8833\n",
      "Epoch 237/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 21340.1307 - val_loss: 16633.1687\n",
      "Epoch 238/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 21257.7412 - val_loss: 17181.0907\n",
      "Epoch 239/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 20899.9569 - val_loss: 18219.8813\n",
      "Epoch 240/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 21073.3430 - val_loss: 17299.2644\n",
      "Epoch 241/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 21657.7159 - val_loss: 17105.1225\n",
      "Epoch 242/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 20736.9746 - val_loss: 20798.5955\n",
      "Epoch 243/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 20355.7497 - val_loss: 16507.8982\n",
      "Epoch 244/1000\n",
      "2304/2304 [==============================] - 1s 510us/sample - loss: 20567.7286 - val_loss: 20939.6114\n",
      "Epoch 245/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 21394.7615 - val_loss: 16379.6078\n",
      "Epoch 246/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 20104.3971 - val_loss: 16366.6618\n",
      "Epoch 247/1000\n",
      "2304/2304 [==============================] - 1s 505us/sample - loss: 20687.8930 - val_loss: 19878.8202\n",
      "Epoch 248/1000\n",
      "2304/2304 [==============================] - 1s 517us/sample - loss: 20930.4913 - val_loss: 16145.8659\n",
      "Epoch 249/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 20190.0410 - val_loss: 16066.6299\n",
      "Epoch 250/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 20514.8601 - val_loss: 17097.0239\n",
      "Epoch 251/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 20327.0836 - val_loss: 18921.1055\n",
      "Epoch 252/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 20391.4029 - val_loss: 16046.4889\n",
      "Epoch 253/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 20673.1577 - val_loss: 25566.8027\n",
      "Epoch 254/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 21491.9382 - val_loss: 16380.3699\n",
      "Epoch 255/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 20895.3963 - val_loss: 17420.4156\n",
      "Epoch 256/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 20294.7704 - val_loss: 16271.8254\n",
      "Epoch 257/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 20491.9727 - val_loss: 16658.8494\n",
      "Epoch 258/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 20086.2639 - val_loss: 15679.3627\n",
      "Epoch 259/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 21141.9408 - val_loss: 15744.2003\n",
      "Epoch 260/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 20496.1258 - val_loss: 16954.4753\n",
      "Epoch 261/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 20223.2753 - val_loss: 16635.8641\n",
      "Epoch 262/1000\n",
      "2304/2304 [==============================] - 1s 545us/sample - loss: 20476.7653 - val_loss: 17225.1371\n",
      "Epoch 263/1000\n",
      "2304/2304 [==============================] - 1s 514us/sample - loss: 20156.7818 - val_loss: 16391.5130\n",
      "Epoch 264/1000\n",
      "2304/2304 [==============================] - 1s 505us/sample - loss: 19906.1036 - val_loss: 19152.6499\n",
      "Epoch 265/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 20546.3438 - val_loss: 18239.4663\n",
      "Epoch 266/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 20597.0776 - val_loss: 15541.3822\n",
      "Epoch 267/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 20203.9459 - val_loss: 15823.5383\n",
      "Epoch 268/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 20446.6344 - val_loss: 15791.7707\n",
      "Epoch 269/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 20372.8632 - val_loss: 15658.4553\n",
      "Epoch 270/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 19949.4939 - val_loss: 15522.8730\n",
      "Epoch 271/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 20356.2966 - val_loss: 24680.8323\n",
      "Epoch 272/1000\n",
      "2304/2304 [==============================] - 1s 560us/sample - loss: 20032.2954 - val_loss: 20645.1183\n",
      "Epoch 273/1000\n",
      "2304/2304 [==============================] - 1s 510us/sample - loss: 19880.5527 - val_loss: 16414.1642\n",
      "Epoch 274/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 19825.5384 - val_loss: 15675.8421\n",
      "Epoch 275/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 19606.9593 - val_loss: 16271.0229\n",
      "Epoch 276/1000\n",
      "2304/2304 [==============================] - 1s 522us/sample - loss: 19890.0068 - val_loss: 15870.4079\n",
      "Epoch 277/1000\n",
      "2304/2304 [==============================] - 1s 505us/sample - loss: 20068.2011 - val_loss: 15406.8122\n",
      "Epoch 278/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 1s 480us/sample - loss: 20217.6465 - val_loss: 15580.8189\n",
      "Epoch 279/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 19728.1291 - val_loss: 17644.7122\n",
      "Epoch 280/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 20696.4248 - val_loss: 18948.0600\n",
      "Epoch 281/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 20588.9868 - val_loss: 17428.5123\n",
      "Epoch 282/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 19589.4809 - val_loss: 20479.4494\n",
      "Epoch 283/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 19989.3347 - val_loss: 24592.2773\n",
      "Epoch 284/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 19601.1443 - val_loss: 17958.3367\n",
      "Epoch 285/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 19954.8026 - val_loss: 18391.1648\n",
      "Epoch 286/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 19540.8160 - val_loss: 16582.8410\n",
      "Epoch 287/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 19150.8726 - val_loss: 16990.6668\n",
      "Epoch 288/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 20187.0309 - val_loss: 19380.9002\n",
      "Epoch 289/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 20068.3503 - val_loss: 15420.8469\n",
      "Epoch 290/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 19737.6568 - val_loss: 17852.5483\n",
      "Epoch 291/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 19427.8907 - val_loss: 15933.9202\n",
      "Epoch 292/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 19486.9367 - val_loss: 16375.1850\n",
      "Epoch 293/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 19500.0959 - val_loss: 16582.1570\n",
      "Epoch 294/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 19180.7617 - val_loss: 15629.0918\n",
      "Epoch 295/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 19490.3953 - val_loss: 16350.7634\n",
      "Epoch 296/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 19007.6486 - val_loss: 15043.1214\n",
      "Epoch 297/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 19679.7610 - val_loss: 15520.7954\n",
      "Epoch 298/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 19237.7800 - val_loss: 15660.4125\n",
      "Epoch 299/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 19662.0068 - val_loss: 14990.3986\n",
      "Epoch 300/1000\n",
      "2304/2304 [==============================] - 1s 512us/sample - loss: 19432.8045 - val_loss: 15538.1014\n",
      "Epoch 301/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 19021.1831 - val_loss: 15618.8178\n",
      "Epoch 302/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 19072.4594 - val_loss: 14749.6235\n",
      "Epoch 303/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 19456.9247 - val_loss: 15435.2798\n",
      "Epoch 304/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 20487.1365 - val_loss: 18499.0873\n",
      "Epoch 305/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 19149.1516 - val_loss: 15228.2504\n",
      "Epoch 306/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 19338.2751 - val_loss: 15659.9636\n",
      "Epoch 307/1000\n",
      "2304/2304 [==============================] - 1s 491us/sample - loss: 20004.1561 - val_loss: 15263.8961\n",
      "Epoch 308/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 19013.0066 - val_loss: 15177.6673\n",
      "Epoch 309/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 19263.0885 - val_loss: 19272.6208\n",
      "Epoch 310/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 18845.4838 - val_loss: 16031.4501\n",
      "Epoch 311/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 19291.9766 - val_loss: 19742.4131\n",
      "Epoch 312/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 19326.5582 - val_loss: 17444.9941\n",
      "Epoch 313/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 19839.3427 - val_loss: 17435.9242\n",
      "Epoch 314/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 18814.3406 - val_loss: 15981.8802\n",
      "Epoch 315/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 18832.7158 - val_loss: 15148.4921\n",
      "Epoch 316/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 20061.5174 - val_loss: 15893.9433\n",
      "Epoch 317/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 19317.8880 - val_loss: 15615.6947\n",
      "Epoch 318/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 19371.2547 - val_loss: 18333.3970\n",
      "Epoch 319/1000\n",
      "2304/2304 [==============================] - 1s 526us/sample - loss: 18998.1560 - val_loss: 15188.8513\n",
      "Epoch 320/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 19068.1932 - val_loss: 14674.0681\n",
      "Epoch 321/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 18654.9144 - val_loss: 15151.3012\n",
      "Epoch 322/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 18963.1104 - val_loss: 16426.6507\n",
      "Epoch 323/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 18437.2927 - val_loss: 15063.9251\n",
      "Epoch 324/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 19247.0431 - val_loss: 15031.2143\n",
      "Epoch 325/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 18782.6633 - val_loss: 14619.5751\n",
      "Epoch 326/1000\n",
      "2304/2304 [==============================] - 1s 509us/sample - loss: 18838.4120 - val_loss: 15649.7828\n",
      "Epoch 327/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 19063.9735 - val_loss: 14745.8680\n",
      "Epoch 328/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 18693.1679 - val_loss: 15096.2225\n",
      "Epoch 329/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 19404.5575 - val_loss: 19391.5469\n",
      "Epoch 330/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 18705.8759 - val_loss: 15438.7388\n",
      "Epoch 331/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 18871.7348 - val_loss: 23949.9566\n",
      "Epoch 332/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 18734.0433 - val_loss: 21178.0121\n",
      "Epoch 333/1000\n",
      "2304/2304 [==============================] - 1s 513us/sample - loss: 18842.9829 - val_loss: 15399.3394\n",
      "Epoch 334/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 18438.9657 - val_loss: 15142.5794\n",
      "Epoch 335/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 18570.1532 - val_loss: 16587.3908\n",
      "Epoch 336/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 18828.0351 - val_loss: 16318.0175\n",
      "Epoch 337/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 19394.8118 - val_loss: 14599.8183\n",
      "Epoch 338/1000\n",
      "2304/2304 [==============================] - 1s 507us/sample - loss: 19172.6256 - val_loss: 17838.8551\n",
      "Epoch 339/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 19634.6823 - val_loss: 14753.2214\n",
      "Epoch 340/1000\n",
      "2304/2304 [==============================] - 1s 536us/sample - loss: 18921.1473 - val_loss: 15951.9208\n",
      "Epoch 341/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 18970.2567 - val_loss: 14861.9569\n",
      "Epoch 342/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 18753.6879 - val_loss: 14692.7467\n",
      "Epoch 343/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 18415.0382 - val_loss: 19540.6750\n",
      "Epoch 344/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 18144.4470 - val_loss: 16378.1502\n",
      "Epoch 345/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 18610.4586 - val_loss: 16250.1309\n",
      "Epoch 346/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 19219.4231 - val_loss: 14549.4145\n",
      "Epoch 347/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 1s 496us/sample - loss: 19091.0828 - val_loss: 14622.8600\n",
      "Epoch 348/1000\n",
      "2304/2304 [==============================] - 1s 483us/sample - loss: 19150.9280 - val_loss: 14781.1418\n",
      "Epoch 349/1000\n",
      "2304/2304 [==============================] - 1s 481us/sample - loss: 18563.3202 - val_loss: 15093.6779\n",
      "Epoch 350/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 18093.9668 - val_loss: 15252.5118\n",
      "Epoch 351/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 18138.9707 - val_loss: 15167.6067\n",
      "Epoch 352/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 19334.5229 - val_loss: 21899.9393\n",
      "Epoch 353/1000\n",
      "2304/2304 [==============================] - 1s 483us/sample - loss: 18669.1184 - val_loss: 15609.1030\n",
      "Epoch 354/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 18894.4427 - val_loss: 16501.3490\n",
      "Epoch 355/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 18361.5302 - val_loss: 16659.7649\n",
      "Epoch 356/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 18781.8967 - val_loss: 16653.1238\n",
      "Epoch 357/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 18386.6063 - val_loss: 17699.9220\n",
      "Epoch 358/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 18904.8022 - val_loss: 19536.5237\n",
      "Epoch 359/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 18809.4641 - val_loss: 15495.8358\n",
      "Epoch 360/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 18234.0326 - val_loss: 15057.5620\n",
      "Epoch 361/1000\n",
      "2304/2304 [==============================] - 1s 491us/sample - loss: 19104.8380 - val_loss: 16186.3192\n",
      "Epoch 362/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 18793.1477 - val_loss: 15995.7899\n",
      "Epoch 363/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 18457.8445 - val_loss: 17282.7255\n",
      "Epoch 364/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 18391.5135 - val_loss: 14584.4088\n",
      "Epoch 365/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 18113.1263 - val_loss: 14551.3736\n",
      "Epoch 366/1000\n",
      "2304/2304 [==============================] - 1s 512us/sample - loss: 18309.5173 - val_loss: 16543.6199\n",
      "Epoch 367/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 19248.4371 - val_loss: 15925.4564\n",
      "Epoch 368/1000\n",
      "2304/2304 [==============================] - 1s 508us/sample - loss: 18612.7591 - val_loss: 15068.7580\n",
      "Epoch 369/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 18484.0205 - val_loss: 14690.8038\n",
      "Epoch 370/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 18205.9269 - val_loss: 21723.3200\n",
      "Epoch 371/1000\n",
      "2304/2304 [==============================] - 1s 516us/sample - loss: 18211.6600 - val_loss: 16744.8658\n",
      "Epoch 372/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 18155.5438 - val_loss: 16555.8599\n",
      "Epoch 373/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 18652.0748 - val_loss: 15148.4544\n",
      "Epoch 374/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 17679.1590 - val_loss: 14675.7560\n",
      "Epoch 375/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 17946.1745 - val_loss: 14668.4582\n",
      "Epoch 376/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 18184.3323 - val_loss: 14693.3752\n",
      "Epoch 377/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 18548.6265 - val_loss: 19995.9303\n",
      "Epoch 378/1000\n",
      "2304/2304 [==============================] - 1s 509us/sample - loss: 18010.5883 - val_loss: 17120.2000\n",
      "Epoch 379/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 18348.2038 - val_loss: 15352.0722\n",
      "Epoch 380/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 17603.3142 - val_loss: 16589.1353\n",
      "Epoch 381/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 18005.1115 - val_loss: 16292.2223\n",
      "Epoch 382/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 18580.6063 - val_loss: 17305.9765\n",
      "Epoch 383/1000\n",
      "2304/2304 [==============================] - 1s 517us/sample - loss: 18407.4348 - val_loss: 14594.8234\n",
      "Epoch 384/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 17318.4937 - val_loss: 16425.9418\n",
      "Epoch 385/1000\n",
      "2304/2304 [==============================] - 1s 524us/sample - loss: 18250.6704 - val_loss: 16165.2162\n",
      "Epoch 386/1000\n",
      "2304/2304 [==============================] - 1s 532us/sample - loss: 18479.2941 - val_loss: 14535.1887\n",
      "Epoch 387/1000\n",
      "2304/2304 [==============================] - 1s 534us/sample - loss: 17894.6088 - val_loss: 15274.4831\n",
      "Epoch 388/1000\n",
      "2304/2304 [==============================] - 1s 558us/sample - loss: 18365.1552 - val_loss: 14412.2192\n",
      "Epoch 389/1000\n",
      "2304/2304 [==============================] - 1s 535us/sample - loss: 17823.1492 - val_loss: 14351.5793\n",
      "Epoch 390/1000\n",
      "2304/2304 [==============================] - 1s 517us/sample - loss: 17782.2974 - val_loss: 14678.0367\n",
      "Epoch 391/1000\n",
      "2304/2304 [==============================] - 1s 510us/sample - loss: 18201.6457 - val_loss: 16126.3203\n",
      "Epoch 392/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 18018.2526 - val_loss: 14275.5543\n",
      "Epoch 393/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 17898.9159 - val_loss: 14864.0619\n",
      "Epoch 394/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 17910.5513 - val_loss: 14397.8300\n",
      "Epoch 395/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 17472.0746 - val_loss: 14269.7409\n",
      "Epoch 396/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 17796.5851 - val_loss: 14210.0479\n",
      "Epoch 397/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 18209.8271 - val_loss: 14441.4057\n",
      "Epoch 398/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 17935.0881 - val_loss: 14493.9683\n",
      "Epoch 399/1000\n",
      "2304/2304 [==============================] - 1s 507us/sample - loss: 18026.3962 - val_loss: 14279.6531\n",
      "Epoch 400/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 17531.5836 - val_loss: 17694.4937\n",
      "Epoch 401/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 17945.2382 - val_loss: 18797.7766\n",
      "Epoch 402/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 18931.1484 - val_loss: 14485.3595\n",
      "Epoch 403/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 17949.7408 - val_loss: 14016.1778\n",
      "Epoch 404/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 17413.6171 - val_loss: 14711.9757\n",
      "Epoch 405/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 18470.9895 - val_loss: 19580.8404\n",
      "Epoch 406/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 18179.4620 - val_loss: 14382.8586\n",
      "Epoch 407/1000\n",
      "2304/2304 [==============================] - 1s 507us/sample - loss: 17973.7152 - val_loss: 15706.9058\n",
      "Epoch 408/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 17610.1475 - val_loss: 19900.6309\n",
      "Epoch 409/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 17541.6374 - val_loss: 14640.0049\n",
      "Epoch 410/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 17431.0604 - val_loss: 14606.0667\n",
      "Epoch 411/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 17547.4440 - val_loss: 14139.3163\n",
      "Epoch 412/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 17225.0073 - val_loss: 14243.3971\n",
      "Epoch 413/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 18482.4948 - val_loss: 14319.1433\n",
      "Epoch 414/1000\n",
      "2304/2304 [==============================] - 1s 521us/sample - loss: 17594.6429 - val_loss: 15712.4624\n",
      "Epoch 415/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 17577.5894 - val_loss: 14622.7335\n",
      "Epoch 416/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 1s 493us/sample - loss: 17858.6398 - val_loss: 13997.8462\n",
      "Epoch 417/1000\n",
      "2304/2304 [==============================] - 1s 507us/sample - loss: 17744.4949 - val_loss: 13878.2441\n",
      "Epoch 418/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 17950.9709 - val_loss: 15904.3755\n",
      "Epoch 419/1000\n",
      "2304/2304 [==============================] - 1s 491us/sample - loss: 17668.5522 - val_loss: 15351.7475\n",
      "Epoch 420/1000\n",
      "2304/2304 [==============================] - 1s 483us/sample - loss: 18052.5855 - val_loss: 19330.3221\n",
      "Epoch 421/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 17181.9798 - val_loss: 15727.2408\n",
      "Epoch 422/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 17745.8906 - val_loss: 14368.8888\n",
      "Epoch 423/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 18746.1957 - val_loss: 17281.9269\n",
      "Epoch 424/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 18215.8148 - val_loss: 15444.1415\n",
      "Epoch 425/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 17247.7497 - val_loss: 14358.5758\n",
      "Epoch 426/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 17623.5876 - val_loss: 15677.3157\n",
      "Epoch 427/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 17327.8126 - val_loss: 15337.6986\n",
      "Epoch 428/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 17596.2691 - val_loss: 14240.3689\n",
      "Epoch 429/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 17496.1162 - val_loss: 14672.1181\n",
      "Epoch 430/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 17840.5792 - val_loss: 16623.3433\n",
      "Epoch 431/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 17669.3610 - val_loss: 17820.9889\n",
      "Epoch 432/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 17747.0321 - val_loss: 14170.9586\n",
      "Epoch 433/1000\n",
      "2304/2304 [==============================] - 1s 508us/sample - loss: 17546.8540 - val_loss: 14517.9988\n",
      "Epoch 434/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 18017.5648 - val_loss: 15057.3037\n",
      "Epoch 435/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 17387.0232 - val_loss: 16898.2933\n",
      "Epoch 436/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 16978.1137 - val_loss: 14943.0143\n",
      "Epoch 437/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 17659.4963 - val_loss: 15476.8036\n",
      "Epoch 438/1000\n",
      "2304/2304 [==============================] - 1s 507us/sample - loss: 17231.2341 - val_loss: 14280.8378\n",
      "Epoch 439/1000\n",
      "2304/2304 [==============================] - 1s 482us/sample - loss: 17891.0306 - val_loss: 16237.6898\n",
      "Epoch 440/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 17431.5866 - val_loss: 22040.2640\n",
      "Epoch 441/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 17347.8430 - val_loss: 20156.2456\n",
      "Epoch 442/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 18334.9560 - val_loss: 16817.2929\n",
      "Epoch 443/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 18209.4924 - val_loss: 16754.2633\n",
      "Epoch 444/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 17510.8283 - val_loss: 14640.5907\n",
      "Epoch 445/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 17714.4941 - val_loss: 16252.3630\n",
      "Epoch 446/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 17840.5966 - val_loss: 14891.2314\n",
      "Epoch 447/1000\n",
      "2304/2304 [==============================] - 1s 491us/sample - loss: 18032.9374 - val_loss: 14421.9001\n",
      "Epoch 448/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 17451.7722 - val_loss: 15482.8363\n",
      "Epoch 449/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 17678.0029 - val_loss: 15487.1550\n",
      "Epoch 450/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 17161.4540 - val_loss: 14951.5563\n",
      "Epoch 451/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 17612.4963 - val_loss: 14156.1932\n",
      "Epoch 452/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 17092.2302 - val_loss: 14717.0834\n",
      "Epoch 453/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 17984.4999 - val_loss: 14408.1169\n",
      "Epoch 454/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 17729.7810 - val_loss: 14815.7132\n",
      "Epoch 455/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 16907.0947 - val_loss: 14877.7827\n",
      "Epoch 456/1000\n",
      "2304/2304 [==============================] - 1s 512us/sample - loss: 17360.8007 - val_loss: 13897.9260\n",
      "Epoch 457/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 17607.2297 - val_loss: 14449.6395\n",
      "Epoch 458/1000\n",
      "2304/2304 [==============================] - 1s 530us/sample - loss: 16894.1310 - val_loss: 15862.1158\n",
      "Epoch 459/1000\n",
      "2304/2304 [==============================] - 1s 491us/sample - loss: 17530.9727 - val_loss: 15028.4112\n",
      "Epoch 460/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 17261.2520 - val_loss: 19012.0109\n",
      "Epoch 461/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 17031.4828 - val_loss: 14319.5461\n",
      "Epoch 462/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 17548.6445 - val_loss: 16160.6538\n",
      "Epoch 463/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 16647.1852 - val_loss: 14599.7884\n",
      "Epoch 464/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 17381.7834 - val_loss: 13969.3463\n",
      "Epoch 465/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 17176.9689 - val_loss: 13958.9420\n",
      "Epoch 466/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 16926.1279 - val_loss: 15500.4730\n",
      "Epoch 467/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 17226.4755 - val_loss: 13703.1176\n",
      "Epoch 468/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 17289.1008 - val_loss: 13855.2446\n",
      "Epoch 469/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 17183.6229 - val_loss: 17517.0242\n",
      "Epoch 470/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 17229.3325 - val_loss: 15269.7404\n",
      "Epoch 471/1000\n",
      "2304/2304 [==============================] - 1s 551us/sample - loss: 17036.1528 - val_loss: 14099.6034\n",
      "Epoch 472/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 16574.8575 - val_loss: 14053.4682\n",
      "Epoch 473/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 17192.7597 - val_loss: 13639.2224\n",
      "Epoch 474/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 16709.1875 - val_loss: 17245.4495\n",
      "Epoch 475/1000\n",
      "2304/2304 [==============================] - 1s 491us/sample - loss: 17002.5698 - val_loss: 18268.4288\n",
      "Epoch 476/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 17714.0021 - val_loss: 16123.1679\n",
      "Epoch 477/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 17735.6320 - val_loss: 14439.4977\n",
      "Epoch 478/1000\n",
      "2304/2304 [==============================] - 1s 509us/sample - loss: 17434.6168 - val_loss: 14419.6573\n",
      "Epoch 479/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 17656.3463 - val_loss: 13986.1846\n",
      "Epoch 480/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 17031.4250 - val_loss: 20011.8225\n",
      "Epoch 481/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 17877.3314 - val_loss: 14379.4160\n",
      "Epoch 482/1000\n",
      "2304/2304 [==============================] - 1s 508us/sample - loss: 16893.3453 - val_loss: 13757.1403\n",
      "Epoch 483/1000\n",
      "2304/2304 [==============================] - 1s 523us/sample - loss: 16712.8070 - val_loss: 14648.5870\n",
      "Epoch 484/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 16883.3692 - val_loss: 14863.5977\n",
      "Epoch 485/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 1s 494us/sample - loss: 17447.9842 - val_loss: 14588.2550\n",
      "Epoch 486/1000\n",
      "2304/2304 [==============================] - 1s 483us/sample - loss: 17713.7349 - val_loss: 14623.9465\n",
      "Epoch 487/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 16546.4448 - val_loss: 16734.7817\n",
      "Epoch 488/1000\n",
      "2304/2304 [==============================] - 1s 491us/sample - loss: 17286.8523 - val_loss: 13733.6928\n",
      "Epoch 489/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 17283.2237 - val_loss: 15606.1923\n",
      "Epoch 490/1000\n",
      "2304/2304 [==============================] - 1s 509us/sample - loss: 17584.8875 - val_loss: 13756.1386\n",
      "Epoch 491/1000\n",
      "2304/2304 [==============================] - 1s 526us/sample - loss: 16407.3924 - val_loss: 14483.8182\n",
      "Epoch 492/1000\n",
      "2304/2304 [==============================] - 1s 534us/sample - loss: 17031.1532 - val_loss: 18642.7856\n",
      "Epoch 493/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 17644.8399 - val_loss: 14303.5354\n",
      "Epoch 494/1000\n",
      "2304/2304 [==============================] - 1s 480us/sample - loss: 16760.6027 - val_loss: 14678.6899\n",
      "Epoch 495/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 17677.3277 - val_loss: 17044.9774\n",
      "Epoch 496/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 17237.6772 - val_loss: 16960.4979\n",
      "Epoch 497/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 16943.7035 - val_loss: 13485.0217\n",
      "Epoch 498/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 16466.1592 - val_loss: 14779.0880\n",
      "Epoch 499/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 17375.6377 - val_loss: 13485.7767\n",
      "Epoch 500/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 16829.9486 - val_loss: 13548.0869\n",
      "Epoch 501/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 16937.0937 - val_loss: 14126.0741\n",
      "Epoch 502/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 16649.2870 - val_loss: 14977.0378\n",
      "Epoch 503/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 16429.6172 - val_loss: 20361.0679\n",
      "Epoch 504/1000\n",
      "2304/2304 [==============================] - 1s 508us/sample - loss: 17009.1808 - val_loss: 14261.7115\n",
      "Epoch 505/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 17192.3892 - val_loss: 15897.9782\n",
      "Epoch 506/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 16535.8517 - val_loss: 13942.0506\n",
      "Epoch 507/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 17095.7940 - val_loss: 15315.8132\n",
      "Epoch 508/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 16661.7930 - val_loss: 16918.4001\n",
      "Epoch 509/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 16479.4281 - val_loss: 14470.1349\n",
      "Epoch 510/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 16552.0514 - val_loss: 14830.6289\n",
      "Epoch 511/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 16578.3632 - val_loss: 14139.6347\n",
      "Epoch 512/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 16457.6223 - val_loss: 13905.8654\n",
      "Epoch 513/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 16873.5305 - val_loss: 14751.1007\n",
      "Epoch 514/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 16986.9557 - val_loss: 17269.6370\n",
      "Epoch 515/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 16672.8029 - val_loss: 16610.8008\n",
      "Epoch 516/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 16277.9235 - val_loss: 14078.1146\n",
      "Epoch 517/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 17491.3796 - val_loss: 13713.6344\n",
      "Epoch 518/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 16235.4785 - val_loss: 24512.7349\n",
      "Epoch 519/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 16864.9355 - val_loss: 13828.9637\n",
      "Epoch 520/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 17059.2056 - val_loss: 19544.6030\n",
      "Epoch 521/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 17006.6553 - val_loss: 17159.4472\n",
      "Epoch 522/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 17062.9672 - val_loss: 16403.2252\n",
      "Epoch 523/1000\n",
      "2304/2304 [==============================] - 1s 505us/sample - loss: 16706.3048 - val_loss: 16034.2785\n",
      "Epoch 524/1000\n",
      "2304/2304 [==============================] - 1s 519us/sample - loss: 16749.1676 - val_loss: 13979.6553\n",
      "Epoch 525/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 16355.4920 - val_loss: 18058.9699\n",
      "Epoch 526/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 16276.2318 - val_loss: 14161.5705\n",
      "Epoch 527/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 16687.2462 - val_loss: 13886.0346\n",
      "Epoch 528/1000\n",
      "2304/2304 [==============================] - 1s 505us/sample - loss: 17217.3465 - val_loss: 14041.1621\n",
      "Epoch 529/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 16552.2587 - val_loss: 13984.3790\n",
      "Epoch 530/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 17194.9033 - val_loss: 13749.1518\n",
      "Epoch 531/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 16223.5136 - val_loss: 14849.9219\n",
      "Epoch 532/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 16475.6031 - val_loss: 16492.0615\n",
      "Epoch 533/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 16491.8746 - val_loss: 13542.6860\n",
      "Epoch 534/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 16644.5176 - val_loss: 14596.3796\n",
      "Epoch 535/1000\n",
      "2304/2304 [==============================] - 1s 510us/sample - loss: 16500.7759 - val_loss: 15960.0271\n",
      "Epoch 536/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 17066.6453 - val_loss: 15659.4633\n",
      "Epoch 537/1000\n",
      "2304/2304 [==============================] - 1s 505us/sample - loss: 17327.8119 - val_loss: 13966.2367\n",
      "Epoch 538/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 16943.0098 - val_loss: 14419.8698\n",
      "Epoch 539/1000\n",
      "2304/2304 [==============================] - 1s 524us/sample - loss: 16463.5683 - val_loss: 14303.8063\n",
      "Epoch 540/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 16670.9575 - val_loss: 13947.6853\n",
      "Epoch 541/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 16521.7590 - val_loss: 13459.1552\n",
      "Epoch 542/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 16707.3361 - val_loss: 13657.5264\n",
      "Epoch 543/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 15916.5330 - val_loss: 14103.5333\n",
      "Epoch 544/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 16483.6019 - val_loss: 17052.3174\n",
      "Epoch 545/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 16713.5371 - val_loss: 14133.9441\n",
      "Epoch 546/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 17042.3750 - val_loss: 14461.1987\n",
      "Epoch 547/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 16148.9814 - val_loss: 13341.9116\n",
      "Epoch 548/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 16488.1447 - val_loss: 15009.5322\n",
      "Epoch 549/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 17857.2442 - val_loss: 18206.6195\n",
      "Epoch 550/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 16703.6030 - val_loss: 14542.3613\n",
      "Epoch 551/1000\n",
      "2304/2304 [==============================] - 1s 508us/sample - loss: 16103.4456 - val_loss: 15564.4144\n",
      "Epoch 552/1000\n",
      "2304/2304 [==============================] - 1s 519us/sample - loss: 16700.5867 - val_loss: 13749.3121\n",
      "Epoch 553/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 16336.6479 - val_loss: 14096.4236\n",
      "Epoch 554/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 1s 484us/sample - loss: 16713.6939 - val_loss: 17174.6470\n",
      "Epoch 555/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 16502.2020 - val_loss: 17420.0688\n",
      "Epoch 556/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 16315.3013 - val_loss: 13813.6409\n",
      "Epoch 557/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 16613.3173 - val_loss: 15340.5970\n",
      "Epoch 558/1000\n",
      "2304/2304 [==============================] - 1s 482us/sample - loss: 16017.7930 - val_loss: 13539.3348\n",
      "Epoch 559/1000\n",
      "2304/2304 [==============================] - 1s 482us/sample - loss: 16082.4083 - val_loss: 14641.8257\n",
      "Epoch 560/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 16579.8886 - val_loss: 13630.6298\n",
      "Epoch 561/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 16585.3433 - val_loss: 13862.8308\n",
      "Epoch 562/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 16590.6744 - val_loss: 13854.2745\n",
      "Epoch 563/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 15714.5256 - val_loss: 14687.3840\n",
      "Epoch 564/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 16400.0851 - val_loss: 13372.3302\n",
      "Epoch 565/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 16511.4722 - val_loss: 13481.2186\n",
      "Epoch 566/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 16259.3221 - val_loss: 14017.4547\n",
      "Epoch 567/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 15861.0809 - val_loss: 16829.9060\n",
      "Epoch 568/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 16417.4901 - val_loss: 14172.6235\n",
      "Epoch 569/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 16160.9948 - val_loss: 14665.9359\n",
      "Epoch 570/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 16397.8622 - val_loss: 16485.3334\n",
      "Epoch 571/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 16540.2855 - val_loss: 13779.4792\n",
      "Epoch 572/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 16103.4166 - val_loss: 14149.0524\n",
      "Epoch 573/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 16285.4584 - val_loss: 15611.9266\n",
      "Epoch 574/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 16002.3014 - val_loss: 13652.5787\n",
      "Epoch 575/1000\n",
      "2304/2304 [==============================] - 1s 540us/sample - loss: 16988.6566 - val_loss: 13985.1833\n",
      "Epoch 576/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 17362.3702 - val_loss: 13936.5326\n",
      "Epoch 577/1000\n",
      "2304/2304 [==============================] - 1s 508us/sample - loss: 16919.7660 - val_loss: 13434.8035\n",
      "Epoch 578/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 16426.5273 - val_loss: 13697.4910\n",
      "Epoch 579/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 15764.4866 - val_loss: 14108.6748\n",
      "Epoch 580/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 17260.4975 - val_loss: 15463.4170\n",
      "Epoch 581/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 16242.2256 - val_loss: 13489.4566\n",
      "Epoch 582/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 15904.5300 - val_loss: 21131.3160\n",
      "Epoch 583/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 16213.3602 - val_loss: 14926.5228\n",
      "Epoch 584/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 16402.6393 - val_loss: 13720.7426\n",
      "Epoch 585/1000\n",
      "2304/2304 [==============================] - 1s 519us/sample - loss: 15910.6802 - val_loss: 14304.8956\n",
      "Epoch 586/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 16399.2966 - val_loss: 14216.8765\n",
      "Epoch 587/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 16447.6377 - val_loss: 16892.3200\n",
      "Epoch 588/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 15954.8032 - val_loss: 15209.7565\n",
      "Epoch 589/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 16614.6498 - val_loss: 15413.8818\n",
      "Epoch 590/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 16551.1772 - val_loss: 13619.8724\n",
      "Epoch 591/1000\n",
      "2304/2304 [==============================] - 1s 523us/sample - loss: 15782.6564 - val_loss: 15306.4047\n",
      "Epoch 592/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 16986.7361 - val_loss: 13304.1851\n",
      "Epoch 593/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 16089.2658 - val_loss: 14431.1293\n",
      "Epoch 594/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 17279.5064 - val_loss: 14349.7336\n",
      "Epoch 595/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 15976.0892 - val_loss: 14847.8767\n",
      "Epoch 596/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 15966.5594 - val_loss: 14792.2770\n",
      "Epoch 597/1000\n",
      "2304/2304 [==============================] - 1s 483us/sample - loss: 16237.0427 - val_loss: 14402.1715\n",
      "Epoch 598/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 16364.4628 - val_loss: 13355.4923\n",
      "Epoch 599/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 16124.2580 - val_loss: 13319.7862\n",
      "Epoch 600/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 15519.4590 - val_loss: 16006.2776\n",
      "Epoch 601/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 15983.6519 - val_loss: 15351.1758\n",
      "Epoch 602/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 16361.9763 - val_loss: 13756.6782\n",
      "Epoch 603/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 15919.1404 - val_loss: 14738.9911\n",
      "Epoch 604/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 15375.1842 - val_loss: 14281.5640\n",
      "Epoch 605/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 16489.6966 - val_loss: 13153.9938\n",
      "Epoch 606/1000\n",
      "2304/2304 [==============================] - 1s 511us/sample - loss: 16092.6485 - val_loss: 19817.6783\n",
      "Epoch 607/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 15910.8322 - val_loss: 16258.1852\n",
      "Epoch 608/1000\n",
      "2304/2304 [==============================] - 1s 510us/sample - loss: 16557.7013 - val_loss: 14384.5476\n",
      "Epoch 609/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 15882.3345 - val_loss: 13560.9837\n",
      "Epoch 610/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 15677.1620 - val_loss: 18116.0151\n",
      "Epoch 611/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 16572.4103 - val_loss: 14077.0486\n",
      "Epoch 612/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 15853.7184 - val_loss: 14090.6116\n",
      "Epoch 613/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 16155.2086 - val_loss: 18131.0674\n",
      "Epoch 614/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 15584.5501 - val_loss: 13327.6564\n",
      "Epoch 615/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 15308.3471 - val_loss: 18040.4149\n",
      "Epoch 616/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 15893.8393 - val_loss: 13457.6593\n",
      "Epoch 617/1000\n",
      "2304/2304 [==============================] - 1s 505us/sample - loss: 16721.1500 - val_loss: 13654.9684\n",
      "Epoch 618/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 15620.6664 - val_loss: 13655.1079\n",
      "Epoch 619/1000\n",
      "2304/2304 [==============================] - 1s 508us/sample - loss: 15899.6125 - val_loss: 14529.8562\n",
      "Epoch 620/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 16124.5176 - val_loss: 13264.0379\n",
      "Epoch 621/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 15954.0834 - val_loss: 14286.7079\n",
      "Epoch 622/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 15821.9780 - val_loss: 15069.8242\n",
      "Epoch 623/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 1s 483us/sample - loss: 16043.5125 - val_loss: 13562.7723\n",
      "Epoch 624/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 15903.5612 - val_loss: 14518.7710\n",
      "Epoch 625/1000\n",
      "2304/2304 [==============================] - 1s 482us/sample - loss: 16083.6299 - val_loss: 15285.8314\n",
      "Epoch 626/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 16341.2674 - val_loss: 14853.7014\n",
      "Epoch 627/1000\n",
      "2304/2304 [==============================] - 1s 483us/sample - loss: 16016.8797 - val_loss: 13463.8490\n",
      "Epoch 628/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 15491.5048 - val_loss: 13238.5734\n",
      "Epoch 629/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 15904.8504 - val_loss: 15450.8245\n",
      "Epoch 630/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 16103.1639 - val_loss: 14083.3109\n",
      "Epoch 631/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 15870.5653 - val_loss: 13408.1564\n",
      "Epoch 632/1000\n",
      "2304/2304 [==============================] - 1s 491us/sample - loss: 15780.9235 - val_loss: 13090.2621\n",
      "Epoch 633/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 15765.9947 - val_loss: 14541.5682\n",
      "Epoch 634/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 16218.5435 - val_loss: 13515.4203\n",
      "Epoch 635/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 15806.9998 - val_loss: 16805.9100\n",
      "Epoch 636/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 16112.4743 - val_loss: 15490.0431\n",
      "Epoch 637/1000\n",
      "2304/2304 [==============================] - 1s 508us/sample - loss: 16626.2321 - val_loss: 14524.8751\n",
      "Epoch 638/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 15883.2246 - val_loss: 13417.1545\n",
      "Epoch 639/1000\n",
      "2304/2304 [==============================] - 1s 609us/sample - loss: 15889.9884 - val_loss: 14721.2165\n",
      "Epoch 640/1000\n",
      "2304/2304 [==============================] - 1s 511us/sample - loss: 16238.1767 - val_loss: 15633.21300s - loss: 1539\n",
      "Epoch 641/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 16010.1973 - val_loss: 16249.1950\n",
      "Epoch 642/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 15924.9733 - val_loss: 14459.8110\n",
      "Epoch 643/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 16009.4486 - val_loss: 14056.6974\n",
      "Epoch 644/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 15953.8215 - val_loss: 16226.0819\n",
      "Epoch 645/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 16676.4460 - val_loss: 13297.7158\n",
      "Epoch 646/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 15730.4611 - val_loss: 19207.4711\n",
      "Epoch 647/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 16244.0618 - val_loss: 13603.9987\n",
      "Epoch 648/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 15289.5324 - val_loss: 13564.2083\n",
      "Epoch 649/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 15646.8677 - val_loss: 13606.4083\n",
      "Epoch 650/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 15767.2914 - val_loss: 15343.9146\n",
      "Epoch 651/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 15928.6782 - val_loss: 13302.6507\n",
      "Epoch 652/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 15612.7119 - val_loss: 17315.0007\n",
      "Epoch 653/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 15780.7040 - val_loss: 13058.9370\n",
      "Epoch 654/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 16095.9263 - val_loss: 16098.7924\n",
      "Epoch 655/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 15935.2790 - val_loss: 13972.5220\n",
      "Epoch 656/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 15868.1841 - val_loss: 14162.6823\n",
      "Epoch 657/1000\n",
      "2304/2304 [==============================] - 1s 580us/sample - loss: 15699.1466 - val_loss: 14095.8303\n",
      "Epoch 658/1000\n",
      "2304/2304 [==============================] - 1s 631us/sample - loss: 15517.3964 - val_loss: 13449.0229\n",
      "Epoch 659/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 15485.5404 - val_loss: 13479.4052\n",
      "Epoch 660/1000\n",
      "2304/2304 [==============================] - 1s 472us/sample - loss: 15859.2816 - val_loss: 13976.6154\n",
      "Epoch 661/1000\n",
      "2304/2304 [==============================] - 1s 466us/sample - loss: 15996.0898 - val_loss: 14001.2613\n",
      "Epoch 662/1000\n",
      "2304/2304 [==============================] - 1s 466us/sample - loss: 16028.7058 - val_loss: 14653.1632\n",
      "Epoch 663/1000\n",
      "2304/2304 [==============================] - 1s 473us/sample - loss: 15530.6254 - val_loss: 15723.6228\n",
      "Epoch 664/1000\n",
      "2304/2304 [==============================] - 1s 468us/sample - loss: 15387.1904 - val_loss: 13599.4018\n",
      "Epoch 665/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 15695.7150 - val_loss: 15985.3227\n",
      "Epoch 666/1000\n",
      "2304/2304 [==============================] - 1s 481us/sample - loss: 15191.4494 - val_loss: 17191.7609\n",
      "Epoch 667/1000\n",
      "2304/2304 [==============================] - 1s 470us/sample - loss: 16064.8466 - val_loss: 14613.3080\n",
      "Epoch 668/1000\n",
      "2304/2304 [==============================] - 1s 467us/sample - loss: 16068.9508 - val_loss: 13665.8965\n",
      "Epoch 669/1000\n",
      "2304/2304 [==============================] - 1s 477us/sample - loss: 15384.3756 - val_loss: 13301.1561\n",
      "Epoch 670/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 16485.5014 - val_loss: 16067.0178\n",
      "Epoch 671/1000\n",
      "2304/2304 [==============================] - 1s 483us/sample - loss: 15661.1083 - val_loss: 13931.2154\n",
      "Epoch 672/1000\n",
      "2304/2304 [==============================] - 1s 471us/sample - loss: 16000.3030 - val_loss: 13202.6250\n",
      "Epoch 673/1000\n",
      "2304/2304 [==============================] - 1s 472us/sample - loss: 15690.5317 - val_loss: 14026.5008\n",
      "Epoch 674/1000\n",
      "2304/2304 [==============================] - 1s 477us/sample - loss: 15665.5779 - val_loss: 20231.1494\n",
      "Epoch 675/1000\n",
      "2304/2304 [==============================] - 1s 583us/sample - loss: 15521.7521 - val_loss: 14161.2972\n",
      "Epoch 676/1000\n",
      "2304/2304 [==============================] - 4s 2ms/sample - loss: 16274.3652 - val_loss: 13947.5895\n",
      "Epoch 677/1000\n",
      "2304/2304 [==============================] - 2s 705us/sample - loss: 15499.5623 - val_loss: 13760.3357\n",
      "Epoch 678/1000\n",
      "2304/2304 [==============================] - 2s 869us/sample - loss: 15683.3346 - val_loss: 14448.6598\n",
      "Epoch 679/1000\n",
      "2304/2304 [==============================] - 7s 3ms/sample - loss: 15711.6875 - val_loss: 14333.2929\n",
      "Epoch 680/1000\n",
      "2304/2304 [==============================] - 1s 521us/sample - loss: 16195.6898 - val_loss: 13327.4327\n",
      "Epoch 681/1000\n",
      "2304/2304 [==============================] - 1s 467us/sample - loss: 15620.0119 - val_loss: 15967.2581\n",
      "Epoch 682/1000\n",
      "2304/2304 [==============================] - 1s 480us/sample - loss: 16449.7748 - val_loss: 13860.0529\n",
      "Epoch 683/1000\n",
      "2304/2304 [==============================] - 1s 440us/sample - loss: 15499.3090 - val_loss: 15900.1183\n",
      "Epoch 684/1000\n",
      "2304/2304 [==============================] - 1s 452us/sample - loss: 15504.5951 - val_loss: 13213.5415\n",
      "Epoch 685/1000\n",
      "2304/2304 [==============================] - 1s 430us/sample - loss: 15120.8791 - val_loss: 14046.7722\n",
      "Epoch 686/1000\n",
      "2304/2304 [==============================] - 1s 432us/sample - loss: 15756.7865 - val_loss: 16613.3238\n",
      "Epoch 687/1000\n",
      "2304/2304 [==============================] - 1s 437us/sample - loss: 15675.7327 - val_loss: 13305.4665\n",
      "Epoch 688/1000\n",
      "2304/2304 [==============================] - 1s 432us/sample - loss: 16640.1744 - val_loss: 13124.7525\n",
      "Epoch 689/1000\n",
      "2304/2304 [==============================] - 1s 439us/sample - loss: 15314.3340 - val_loss: 13380.5745\n",
      "Epoch 690/1000\n",
      "2304/2304 [==============================] - 1s 430us/sample - loss: 15705.5180 - val_loss: 13782.9047\n",
      "Epoch 691/1000\n",
      "2304/2304 [==============================] - 1s 430us/sample - loss: 15358.4153 - val_loss: 15731.4257\n",
      "Epoch 692/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 1s 427us/sample - loss: 15624.1193 - val_loss: 13282.4605\n",
      "Epoch 693/1000\n",
      "2304/2304 [==============================] - 1s 436us/sample - loss: 15324.2036 - val_loss: 13149.3997\n",
      "Epoch 694/1000\n",
      "2304/2304 [==============================] - 1s 433us/sample - loss: 15570.4064 - val_loss: 14406.5906\n",
      "Epoch 695/1000\n",
      "2304/2304 [==============================] - 1s 430us/sample - loss: 15729.6734 - val_loss: 14918.4821\n",
      "Epoch 696/1000\n",
      "2304/2304 [==============================] - 1s 432us/sample - loss: 15184.7566 - val_loss: 13231.7180\n",
      "Epoch 697/1000\n",
      "2304/2304 [==============================] - 1s 428us/sample - loss: 15144.4683 - val_loss: 13929.0606\n",
      "Epoch 698/1000\n",
      "2304/2304 [==============================] - 1s 521us/sample - loss: 15517.0002 - val_loss: 19117.8182\n",
      "Epoch 699/1000\n",
      "2304/2304 [==============================] - 1s 454us/sample - loss: 15671.9990 - val_loss: 13222.4760\n",
      "Epoch 700/1000\n",
      "2304/2304 [==============================] - 1s 430us/sample - loss: 15712.9707 - val_loss: 13103.3326\n",
      "Epoch 701/1000\n",
      "2304/2304 [==============================] - 1s 428us/sample - loss: 15451.3324 - val_loss: 13963.3242\n",
      "Epoch 702/1000\n",
      "2304/2304 [==============================] - 1s 426us/sample - loss: 15604.9818 - val_loss: 14323.0729\n",
      "Epoch 703/1000\n",
      "2304/2304 [==============================] - 1s 431us/sample - loss: 16458.8507 - val_loss: 14551.8909\n",
      "Epoch 704/1000\n",
      "2304/2304 [==============================] - 1s 429us/sample - loss: 15616.4574 - val_loss: 13745.1155\n",
      "Epoch 705/1000\n",
      "2304/2304 [==============================] - 1s 430us/sample - loss: 15260.1131 - val_loss: 14559.8119\n",
      "Epoch 706/1000\n",
      "2304/2304 [==============================] - 1s 429us/sample - loss: 15282.4943 - val_loss: 14124.7765\n",
      "Epoch 707/1000\n",
      "2304/2304 [==============================] - 1s 429us/sample - loss: 15895.6096 - val_loss: 13611.1647\n",
      "Epoch 708/1000\n",
      "2304/2304 [==============================] - 1s 429us/sample - loss: 15460.1896 - val_loss: 13503.6367\n",
      "Epoch 709/1000\n",
      "2304/2304 [==============================] - 1s 431us/sample - loss: 15398.3207 - val_loss: 13326.0433\n",
      "Epoch 710/1000\n",
      "2304/2304 [==============================] - 1s 429us/sample - loss: 15998.3291 - val_loss: 15068.3284\n",
      "Epoch 711/1000\n",
      "2304/2304 [==============================] - 1s 431us/sample - loss: 15636.4787 - val_loss: 13501.4370\n",
      "Epoch 712/1000\n",
      "2304/2304 [==============================] - 1s 428us/sample - loss: 16365.5012 - val_loss: 13143.8736\n",
      "Epoch 713/1000\n",
      "2304/2304 [==============================] - 1s 451us/sample - loss: 15495.2982 - val_loss: 13132.3621\n",
      "Epoch 714/1000\n",
      "2304/2304 [==============================] - 1s 439us/sample - loss: 16046.2338 - val_loss: 13253.9261\n",
      "Epoch 715/1000\n",
      "2304/2304 [==============================] - 1s 441us/sample - loss: 15826.3242 - val_loss: 13209.0682\n",
      "Epoch 716/1000\n",
      "2304/2304 [==============================] - 1s 430us/sample - loss: 15725.7754 - val_loss: 13755.8142\n",
      "Epoch 717/1000\n",
      "2304/2304 [==============================] - 1s 431us/sample - loss: 16406.5889 - val_loss: 15815.9541\n",
      "Epoch 718/1000\n",
      "2304/2304 [==============================] - 1s 439us/sample - loss: 15764.3034 - val_loss: 13498.9865\n",
      "Epoch 719/1000\n",
      "2304/2304 [==============================] - 294s 128ms/sample - loss: 15096.8786 - val_loss: 16101.1725\n",
      "Epoch 720/1000\n",
      "2304/2304 [==============================] - 1s 614us/sample - loss: 15094.6314 - val_loss: 13728.8813\n",
      "Epoch 721/1000\n",
      "2304/2304 [==============================] - 1s 640us/sample - loss: 15049.3687 - val_loss: 13174.3375\n",
      "Epoch 722/1000\n",
      "2304/2304 [==============================] - 3s 1ms/sample - loss: 15539.1468 - val_loss: 13102.9046\n",
      "Epoch 723/1000\n",
      "2304/2304 [==============================] - 2s 799us/sample - loss: 15366.8315 - val_loss: 13046.6746\n",
      "Epoch 724/1000\n",
      "2304/2304 [==============================] - 1s 644us/sample - loss: 15351.5043 - val_loss: 16219.6935\n",
      "Epoch 725/1000\n",
      "2304/2304 [==============================] - 2s 945us/sample - loss: 15260.1936 - val_loss: 14450.8944\n",
      "Epoch 726/1000\n",
      "2304/2304 [==============================] - 1s 532us/sample - loss: 15884.5447 - val_loss: 13868.4951\n",
      "Epoch 727/1000\n",
      "2304/2304 [==============================] - 1s 648us/sample - loss: 15108.6791 - val_loss: 16947.1119\n",
      "Epoch 728/1000\n",
      "2304/2304 [==============================] - 1s 607us/sample - loss: 14910.2263 - val_loss: 13654.5765\n",
      "Epoch 729/1000\n",
      "2304/2304 [==============================] - 1s 572us/sample - loss: 14894.4315 - val_loss: 15614.0927\n",
      "Epoch 730/1000\n",
      "2304/2304 [==============================] - 1s 607us/sample - loss: 15400.6495 - val_loss: 13365.8784\n",
      "Epoch 731/1000\n",
      "2304/2304 [==============================] - 1s 591us/sample - loss: 15348.8921 - val_loss: 13217.3659\n",
      "Epoch 732/1000\n",
      "2304/2304 [==============================] - 2s 853us/sample - loss: 15222.3394 - val_loss: 15235.6625\n",
      "Epoch 733/1000\n",
      "2304/2304 [==============================] - 1s 549us/sample - loss: 14533.8354 - val_loss: 14038.4544\n",
      "Epoch 734/1000\n",
      "2304/2304 [==============================] - 1s 556us/sample - loss: 15357.9867 - val_loss: 13620.6406\n",
      "Epoch 735/1000\n",
      "2304/2304 [==============================] - 1s 558us/sample - loss: 15568.9122 - val_loss: 13639.4552\n",
      "Epoch 736/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 15046.0396 - val_loss: 13659.8693\n",
      "Epoch 737/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 14711.9991 - val_loss: 15139.4143\n",
      "Epoch 738/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 15647.9486 - val_loss: 16346.5760\n",
      "Epoch 739/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 15503.0496 - val_loss: 15750.3490\n",
      "Epoch 740/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 14864.8871 - val_loss: 14861.6616\n",
      "Epoch 741/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 15703.2642 - val_loss: 13276.8835\n",
      "Epoch 742/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 15561.4493 - val_loss: 13341.6372\n",
      "Epoch 743/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 15002.4988 - val_loss: 14329.7129\n",
      "Epoch 744/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 15301.1849 - val_loss: 13539.9907\n",
      "Epoch 745/1000\n",
      "2304/2304 [==============================] - 1s 571us/sample - loss: 15520.1811 - val_loss: 13292.0001\n",
      "Epoch 746/1000\n",
      "2304/2304 [==============================] - 1s 592us/sample - loss: 15767.4594 - val_loss: 14257.5853\n",
      "Epoch 747/1000\n",
      "2304/2304 [==============================] - 1s 590us/sample - loss: 15663.5935 - val_loss: 14213.6087\n",
      "Epoch 748/1000\n",
      "2304/2304 [==============================] - 1s 580us/sample - loss: 15219.5197 - val_loss: 14032.8001\n",
      "Epoch 749/1000\n",
      "2304/2304 [==============================] - 1s 572us/sample - loss: 15330.0347 - val_loss: 13938.0712\n",
      "Epoch 750/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 15227.9706 - val_loss: 14407.9009\n",
      "Epoch 751/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 15732.9701 - val_loss: 16006.0948\n",
      "Epoch 752/1000\n",
      "2304/2304 [==============================] - 1s 505us/sample - loss: 15571.6307 - val_loss: 13589.0880\n",
      "Epoch 753/1000\n",
      "2304/2304 [==============================] - 1s 526us/sample - loss: 15368.2272 - val_loss: 12914.9468\n",
      "Epoch 754/1000\n",
      "2304/2304 [==============================] - 1s 505us/sample - loss: 15107.6755 - val_loss: 13393.5706\n",
      "Epoch 755/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 15127.5163 - val_loss: 13417.8568\n",
      "Epoch 756/1000\n",
      "2304/2304 [==============================] - 1s 508us/sample - loss: 15319.8204 - val_loss: 14340.9364\n",
      "Epoch 757/1000\n",
      "2304/2304 [==============================] - 1s 533us/sample - loss: 15006.5466 - val_loss: 14845.3865\n",
      "Epoch 758/1000\n",
      "2304/2304 [==============================] - 1s 512us/sample - loss: 15915.3521 - val_loss: 13133.3267\n",
      "Epoch 759/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 15003.4397 - val_loss: 16705.4564\n",
      "Epoch 760/1000\n",
      "2304/2304 [==============================] - 1s 509us/sample - loss: 15551.9402 - val_loss: 13671.1783\n",
      "Epoch 761/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 1s 483us/sample - loss: 15122.8561 - val_loss: 15445.2869\n",
      "Epoch 762/1000\n",
      "2304/2304 [==============================] - 1s 567us/sample - loss: 14759.9176 - val_loss: 13967.7473\n",
      "Epoch 763/1000\n",
      "2304/2304 [==============================] - 1s 607us/sample - loss: 15697.7533 - val_loss: 13598.0831\n",
      "Epoch 764/1000\n",
      "2304/2304 [==============================] - 1s 555us/sample - loss: 15341.3001 - val_loss: 13520.7713\n",
      "Epoch 765/1000\n",
      "2304/2304 [==============================] - 1s 516us/sample - loss: 15293.8481 - val_loss: 13402.6230\n",
      "Epoch 766/1000\n",
      "2304/2304 [==============================] - 1s 543us/sample - loss: 15221.7373 - val_loss: 13005.1626\n",
      "Epoch 767/1000\n",
      "2304/2304 [==============================] - 1s 507us/sample - loss: 15091.4017 - val_loss: 17340.0254\n",
      "Epoch 768/1000\n",
      "2304/2304 [==============================] - 1s 532us/sample - loss: 15369.1744 - val_loss: 13360.5715\n",
      "Epoch 769/1000\n",
      "2304/2304 [==============================] - 1s 533us/sample - loss: 15077.4268 - val_loss: 13537.7700\n",
      "Epoch 770/1000\n",
      "2304/2304 [==============================] - 1s 532us/sample - loss: 15197.2716 - val_loss: 17852.9036\n",
      "Epoch 771/1000\n",
      "2304/2304 [==============================] - 1s 549us/sample - loss: 16032.4172 - val_loss: 17126.7723\n",
      "Epoch 772/1000\n",
      "2304/2304 [==============================] - 1s 526us/sample - loss: 14989.9066 - val_loss: 16573.2722\n",
      "Epoch 773/1000\n",
      "2304/2304 [==============================] - 1s 542us/sample - loss: 15866.4992 - val_loss: 14369.1724\n",
      "Epoch 774/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 14993.0531 - val_loss: 14444.9317\n",
      "Epoch 775/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 16561.6366 - val_loss: 13054.0974\n",
      "Epoch 776/1000\n",
      "2304/2304 [==============================] - 1s 534us/sample - loss: 15588.4554 - val_loss: 13156.4085\n",
      "Epoch 777/1000\n",
      "2304/2304 [==============================] - 1s 542us/sample - loss: 15534.9328 - val_loss: 13410.5543\n",
      "Epoch 778/1000\n",
      "2304/2304 [==============================] - 1s 522us/sample - loss: 15244.5719 - val_loss: 13706.5312\n",
      "Epoch 779/1000\n",
      "2304/2304 [==============================] - 1s 510us/sample - loss: 15334.6854 - val_loss: 13184.9966\n",
      "Epoch 780/1000\n",
      "2304/2304 [==============================] - 1s 523us/sample - loss: 15578.6440 - val_loss: 12835.5802\n",
      "Epoch 781/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 15851.9017 - val_loss: 20229.9242\n",
      "Epoch 782/1000\n",
      "2304/2304 [==============================] - 1s 567us/sample - loss: 15067.9097 - val_loss: 14727.9337\n",
      "Epoch 783/1000\n",
      "2304/2304 [==============================] - 1s 552us/sample - loss: 15049.0988 - val_loss: 13483.0890\n",
      "Epoch 784/1000\n",
      "2304/2304 [==============================] - 1s 535us/sample - loss: 15178.3334 - val_loss: 13054.4824\n",
      "Epoch 785/1000\n",
      "2304/2304 [==============================] - 1s 552us/sample - loss: 15014.5802 - val_loss: 12758.0209\n",
      "Epoch 786/1000\n",
      "2304/2304 [==============================] - 1s 521us/sample - loss: 14515.8428 - val_loss: 13597.7051\n",
      "Epoch 787/1000\n",
      "2304/2304 [==============================] - 1s 544us/sample - loss: 15112.1421 - val_loss: 13605.7596\n",
      "Epoch 788/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 15265.5805 - val_loss: 16521.6259\n",
      "Epoch 789/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 14873.1703 - val_loss: 13242.6713\n",
      "Epoch 790/1000\n",
      "2304/2304 [==============================] - 1s 507us/sample - loss: 14810.8924 - val_loss: 14071.6522\n",
      "Epoch 791/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 14822.2480 - val_loss: 13019.5360\n",
      "Epoch 792/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 15131.0863 - val_loss: 13784.7679\n",
      "Epoch 793/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 14947.0418 - val_loss: 13051.5518\n",
      "Epoch 794/1000\n",
      "2304/2304 [==============================] - 1s 508us/sample - loss: 15058.4467 - val_loss: 14876.8202\n",
      "Epoch 795/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 14877.1446 - val_loss: 14682.9533\n",
      "Epoch 796/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 15124.3733 - val_loss: 13051.0007\n",
      "Epoch 797/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 15346.2991 - val_loss: 13543.5058\n",
      "Epoch 798/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 14744.3931 - val_loss: 14628.4469\n",
      "Epoch 799/1000\n",
      "2304/2304 [==============================] - 1s 511us/sample - loss: 15684.4049 - val_loss: 13921.9133\n",
      "Epoch 800/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 15342.5325 - val_loss: 14104.6347\n",
      "Epoch 801/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 15188.9929 - val_loss: 13638.9624\n",
      "Epoch 802/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 16099.0878 - val_loss: 14966.8437\n",
      "Epoch 803/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 14978.0472 - val_loss: 12935.5123\n",
      "Epoch 804/1000\n",
      "2304/2304 [==============================] - 1s 527us/sample - loss: 14820.8652 - val_loss: 13586.2914\n",
      "Epoch 805/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 15123.8211 - val_loss: 13759.1104\n",
      "Epoch 806/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 15052.3255 - val_loss: 13031.4623\n",
      "Epoch 807/1000\n",
      "2304/2304 [==============================] - 1s 516us/sample - loss: 15324.6947 - val_loss: 15015.7397\n",
      "Epoch 808/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 15521.8214 - val_loss: 13594.4414\n",
      "Epoch 809/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 15106.6055 - val_loss: 13117.3809\n",
      "Epoch 810/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 15193.5656 - val_loss: 12991.8402\n",
      "Epoch 811/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 15327.8396 - val_loss: 14490.3614\n",
      "Epoch 812/1000\n",
      "2304/2304 [==============================] - 1s 509us/sample - loss: 15098.7940 - val_loss: 14819.7778\n",
      "Epoch 813/1000\n",
      "2304/2304 [==============================] - 1s 509us/sample - loss: 15240.9034 - val_loss: 12952.7901\n",
      "Epoch 814/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 15576.1692 - val_loss: 13257.2084\n",
      "Epoch 815/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 15440.2883 - val_loss: 13347.1425\n",
      "Epoch 816/1000\n",
      "2304/2304 [==============================] - 1s 516us/sample - loss: 15662.5167 - val_loss: 15619.9231\n",
      "Epoch 817/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 15366.9905 - val_loss: 13352.3942\n",
      "Epoch 818/1000\n",
      "2304/2304 [==============================] - 1s 529us/sample - loss: 15047.8507 - val_loss: 14119.7410\n",
      "Epoch 819/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 15017.3493 - val_loss: 13422.5114\n",
      "Epoch 820/1000\n",
      "2304/2304 [==============================] - 1s 509us/sample - loss: 14693.1014 - val_loss: 14104.2872\n",
      "Epoch 821/1000\n",
      "2304/2304 [==============================] - 1s 499us/sample - loss: 14739.5810 - val_loss: 13256.0029\n",
      "Epoch 822/1000\n",
      "2304/2304 [==============================] - 1s 507us/sample - loss: 15314.6616 - val_loss: 14371.0882\n",
      "Epoch 823/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 15227.2358 - val_loss: 13391.3687\n",
      "Epoch 824/1000\n",
      "2304/2304 [==============================] - 1s 508us/sample - loss: 14582.5425 - val_loss: 13218.4780\n",
      "Epoch 825/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 14914.3748 - val_loss: 12962.2466\n",
      "Epoch 826/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 14951.3070 - val_loss: 18110.9171\n",
      "Epoch 827/1000\n",
      "2304/2304 [==============================] - 1s 505us/sample - loss: 14726.0695 - val_loss: 13056.9666\n",
      "Epoch 828/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 14789.2357 - val_loss: 15656.2545\n",
      "Epoch 829/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 14642.9977 - val_loss: 14005.5160\n",
      "Epoch 830/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 1s 481us/sample - loss: 15032.7311 - val_loss: 14297.0222\n",
      "Epoch 831/1000\n",
      "2304/2304 [==============================] - 1s 480us/sample - loss: 14681.7560 - val_loss: 13254.0469\n",
      "Epoch 832/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 14739.5985 - val_loss: 13737.5505\n",
      "Epoch 833/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 15821.7787 - val_loss: 15130.6698\n",
      "Epoch 834/1000\n",
      "2304/2304 [==============================] - 1s 482us/sample - loss: 14817.5747 - val_loss: 14792.3994\n",
      "Epoch 835/1000\n",
      "2304/2304 [==============================] - 1s 477us/sample - loss: 15700.1062 - val_loss: 14380.7635\n",
      "Epoch 836/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 15078.0946 - val_loss: 12968.9987\n",
      "Epoch 837/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 14164.4347 - val_loss: 13279.0803\n",
      "Epoch 838/1000\n",
      "2304/2304 [==============================] - 1s 480us/sample - loss: 15739.9486 - val_loss: 14087.2960\n",
      "Epoch 839/1000\n",
      "2304/2304 [==============================] - 1s 477us/sample - loss: 15502.7664 - val_loss: 12970.4172\n",
      "Epoch 840/1000\n",
      "2304/2304 [==============================] - 1s 477us/sample - loss: 14709.5327 - val_loss: 14311.5503\n",
      "Epoch 841/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 14859.1304 - val_loss: 16450.2896\n",
      "Epoch 842/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 14984.0174 - val_loss: 13205.5909\n",
      "Epoch 843/1000\n",
      "2304/2304 [==============================] - 1s 479us/sample - loss: 15357.3744 - val_loss: 13244.4975\n",
      "Epoch 844/1000\n",
      "2304/2304 [==============================] - 1s 480us/sample - loss: 15351.5508 - val_loss: 13165.9629\n",
      "Epoch 845/1000\n",
      "2304/2304 [==============================] - 1s 479us/sample - loss: 14730.3924 - val_loss: 16758.9246\n",
      "Epoch 846/1000\n",
      "2304/2304 [==============================] - 1s 483us/sample - loss: 14968.9612 - val_loss: 15244.8504\n",
      "Epoch 847/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 14459.2491 - val_loss: 13932.2835\n",
      "Epoch 848/1000\n",
      "2304/2304 [==============================] - 1s 481us/sample - loss: 14489.5376 - val_loss: 17728.8265\n",
      "Epoch 849/1000\n",
      "2304/2304 [==============================] - 1s 481us/sample - loss: 14472.5208 - val_loss: 14952.6377\n",
      "Epoch 850/1000\n",
      "2304/2304 [==============================] - 1s 479us/sample - loss: 14804.7374 - val_loss: 12961.8726\n",
      "Epoch 851/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 14550.3038 - val_loss: 13868.7566\n",
      "Epoch 852/1000\n",
      "2304/2304 [==============================] - 1s 480us/sample - loss: 15378.8448 - val_loss: 13522.2728\n",
      "Epoch 853/1000\n",
      "2304/2304 [==============================] - 1s 480us/sample - loss: 14391.1355 - val_loss: 15404.0365\n",
      "Epoch 854/1000\n",
      "2304/2304 [==============================] - 1s 481us/sample - loss: 14763.9951 - val_loss: 13703.6341\n",
      "Epoch 855/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 14907.1513 - val_loss: 13275.2773\n",
      "Epoch 856/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 14771.5796 - val_loss: 13932.5778\n",
      "Epoch 857/1000\n",
      "2304/2304 [==============================] - 1s 482us/sample - loss: 14844.1330 - val_loss: 13350.9061\n",
      "Epoch 858/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 14832.7029 - val_loss: 13352.9564\n",
      "Epoch 859/1000\n",
      "2304/2304 [==============================] - 1s 514us/sample - loss: 15017.9137 - val_loss: 12976.6548\n",
      "Epoch 860/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 15252.8569 - val_loss: 14434.8437\n",
      "Epoch 861/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 15457.0461 - val_loss: 13309.4256\n",
      "Epoch 862/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 15204.8193 - val_loss: 16299.1447\n",
      "Epoch 863/1000\n",
      "2304/2304 [==============================] - 1s 481us/sample - loss: 14779.0658 - val_loss: 14593.0623\n",
      "Epoch 864/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 14941.7924 - val_loss: 13307.6217\n",
      "Epoch 865/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 14777.8459 - val_loss: 19634.4020\n",
      "Epoch 866/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 14610.2438 - val_loss: 14744.9906\n",
      "Epoch 867/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 14896.3108 - val_loss: 13570.8751\n",
      "Epoch 868/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 14952.7230 - val_loss: 13437.7051\n",
      "Epoch 869/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 14576.2127 - val_loss: 13198.2664\n",
      "Epoch 870/1000\n",
      "2304/2304 [==============================] - 1s 483us/sample - loss: 14309.5202 - val_loss: 15684.9612\n",
      "Epoch 871/1000\n",
      "2304/2304 [==============================] - 1s 491us/sample - loss: 14618.8600 - val_loss: 14459.1554\n",
      "Epoch 872/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 14664.7920 - val_loss: 16436.9027\n",
      "Epoch 873/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 14859.7135 - val_loss: 13817.3717\n",
      "Epoch 874/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 14452.8053 - val_loss: 15711.7872\n",
      "Epoch 875/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 15275.0781 - val_loss: 14171.9136\n",
      "Epoch 876/1000\n",
      "2304/2304 [==============================] - 1s 521us/sample - loss: 15142.8995 - val_loss: 13654.0496\n",
      "Epoch 877/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 14859.0931 - val_loss: 13348.5650\n",
      "Epoch 878/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 15016.9419 - val_loss: 13776.5473\n",
      "Epoch 879/1000\n",
      "2304/2304 [==============================] - 1s 482us/sample - loss: 15334.1042 - val_loss: 14297.0378\n",
      "Epoch 880/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 14937.3297 - val_loss: 19762.1609\n",
      "Epoch 881/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 14842.8621 - val_loss: 13733.4764\n",
      "Epoch 882/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 14693.1389 - val_loss: 14000.5868\n",
      "Epoch 883/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 14796.5285 - val_loss: 13061.5373\n",
      "Epoch 884/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 14289.1985 - val_loss: 12990.7242\n",
      "Epoch 885/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 15222.3201 - val_loss: 15282.0993\n",
      "Epoch 886/1000\n",
      "2304/2304 [==============================] - 1s 509us/sample - loss: 14204.5504 - val_loss: 13523.3720\n",
      "Epoch 887/1000\n",
      "2304/2304 [==============================] - 1s 514us/sample - loss: 14417.2313 - val_loss: 13465.9038\n",
      "Epoch 888/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 14489.6134 - val_loss: 12984.4482\n",
      "Epoch 889/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 14643.4634 - val_loss: 13164.1044\n",
      "Epoch 890/1000\n",
      "2304/2304 [==============================] - 1s 532us/sample - loss: 14498.4183 - val_loss: 13384.4008\n",
      "Epoch 891/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 14206.1667 - val_loss: 13997.4366\n",
      "Epoch 892/1000\n",
      "2304/2304 [==============================] - 1s 482us/sample - loss: 15557.4024 - val_loss: 14826.8127\n",
      "Epoch 893/1000\n",
      "2304/2304 [==============================] - 1s 493us/sample - loss: 14697.4726 - val_loss: 12956.5334\n",
      "Epoch 894/1000\n",
      "2304/2304 [==============================] - 1s 496us/sample - loss: 14945.7412 - val_loss: 16782.6609\n",
      "Epoch 895/1000\n",
      "2304/2304 [==============================] - 1s 507us/sample - loss: 15123.1393 - val_loss: 13325.1350\n",
      "Epoch 896/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 15220.2626 - val_loss: 15144.9679\n",
      "Epoch 897/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 15157.3236 - val_loss: 13998.6909\n",
      "Epoch 898/1000\n",
      "2304/2304 [==============================] - 1s 494us/sample - loss: 14754.9347 - val_loss: 15129.5891\n",
      "Epoch 899/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 1s 485us/sample - loss: 15659.4843 - val_loss: 15786.4620\n",
      "Epoch 900/1000\n",
      "2304/2304 [==============================] - 1s 478us/sample - loss: 14682.8179 - val_loss: 12797.9885\n",
      "Epoch 901/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 14574.6288 - val_loss: 13507.6166\n",
      "Epoch 902/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 14146.2343 - val_loss: 13391.4518\n",
      "Epoch 903/1000\n",
      "2304/2304 [==============================] - 1s 480us/sample - loss: 15051.8946 - val_loss: 16467.0598\n",
      "Epoch 904/1000\n",
      "2304/2304 [==============================] - 1s 490us/sample - loss: 14527.2116 - val_loss: 15494.2211\n",
      "Epoch 905/1000\n",
      "2304/2304 [==============================] - 1s 539us/sample - loss: 14600.5081 - val_loss: 13262.0212\n",
      "Epoch 906/1000\n",
      "2304/2304 [==============================] - 1s 479us/sample - loss: 14341.3718 - val_loss: 14600.7597\n",
      "Epoch 907/1000\n",
      "2304/2304 [==============================] - 1s 521us/sample - loss: 14332.1857 - val_loss: 13338.7004\n",
      "Epoch 908/1000\n",
      "2304/2304 [==============================] - 1s 479us/sample - loss: 14293.6485 - val_loss: 13045.0035\n",
      "Epoch 909/1000\n",
      "2304/2304 [==============================] - 1s 483us/sample - loss: 14227.7537 - val_loss: 13514.5594\n",
      "Epoch 910/1000\n",
      "2304/2304 [==============================] - 1s 479us/sample - loss: 14614.9992 - val_loss: 14291.0985\n",
      "Epoch 911/1000\n",
      "2304/2304 [==============================] - 1s 489us/sample - loss: 13923.1783 - val_loss: 14818.0344\n",
      "Epoch 912/1000\n",
      "2304/2304 [==============================] - 1s 520us/sample - loss: 14192.6607 - val_loss: 12945.1108\n",
      "Epoch 913/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 14284.1224 - val_loss: 13272.2064\n",
      "Epoch 914/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 14323.2158 - val_loss: 14518.1241\n",
      "Epoch 915/1000\n",
      "2304/2304 [==============================] - 1s 478us/sample - loss: 15141.9499 - val_loss: 14290.2515\n",
      "Epoch 916/1000\n",
      "2304/2304 [==============================] - 1s 475us/sample - loss: 14494.9613 - val_loss: 13105.8549\n",
      "Epoch 917/1000\n",
      "2304/2304 [==============================] - 1s 486us/sample - loss: 14287.0867 - val_loss: 13561.3554\n",
      "Epoch 918/1000\n",
      "2304/2304 [==============================] - 1s 487us/sample - loss: 15295.1948 - val_loss: 13251.1448\n",
      "Epoch 919/1000\n",
      "2304/2304 [==============================] - 1s 478us/sample - loss: 14604.9887 - val_loss: 13107.2781\n",
      "Epoch 920/1000\n",
      "2304/2304 [==============================] - 1s 479us/sample - loss: 13974.9097 - val_loss: 16553.3935\n",
      "Epoch 921/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 14896.2490 - val_loss: 15563.2288\n",
      "Epoch 922/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 15669.9474 - val_loss: 14272.8110\n",
      "Epoch 923/1000\n",
      "2304/2304 [==============================] - 1s 480us/sample - loss: 14376.0840 - val_loss: 13563.5119\n",
      "Epoch 924/1000\n",
      "2304/2304 [==============================] - 1s 479us/sample - loss: 14306.6372 - val_loss: 13541.6059\n",
      "Epoch 925/1000\n",
      "2304/2304 [==============================] - 1s 476us/sample - loss: 13959.2964 - val_loss: 16486.7870\n",
      "Epoch 926/1000\n",
      "2304/2304 [==============================] - 1s 492us/sample - loss: 14451.0067 - val_loss: 15205.4792\n",
      "Epoch 927/1000\n",
      "2304/2304 [==============================] - 1s 507us/sample - loss: 15086.8929 - val_loss: 14859.0240\n",
      "Epoch 928/1000\n",
      "2304/2304 [==============================] - 1s 483us/sample - loss: 14293.7822 - val_loss: 14292.4627\n",
      "Epoch 929/1000\n",
      "2304/2304 [==============================] - 1s 479us/sample - loss: 14482.3435 - val_loss: 13209.2783\n",
      "Epoch 930/1000\n",
      "2304/2304 [==============================] - 1s 595us/sample - loss: 14894.2844 - val_loss: 17134.7192\n",
      "Epoch 931/1000\n",
      "2304/2304 [==============================] - 1s 610us/sample - loss: 14656.4999 - val_loss: 13179.6397\n",
      "Epoch 932/1000\n",
      "2304/2304 [==============================] - 1s 583us/sample - loss: 14909.9162 - val_loss: 13342.2719\n",
      "Epoch 933/1000\n",
      "2304/2304 [==============================] - 1s 521us/sample - loss: 14850.2007 - val_loss: 13024.2184\n",
      "Epoch 934/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 15072.4148 - val_loss: 15332.4530\n",
      "Epoch 935/1000\n",
      "2304/2304 [==============================] - 1s 500us/sample - loss: 14184.4427 - val_loss: 12844.8957\n",
      "Epoch 936/1000\n",
      "2304/2304 [==============================] - 1s 520us/sample - loss: 14250.2081 - val_loss: 13145.2572\n",
      "Epoch 937/1000\n",
      "2304/2304 [==============================] - 1s 539us/sample - loss: 14664.5436 - val_loss: 13359.2524\n",
      "Epoch 938/1000\n",
      "2304/2304 [==============================] - 1s 511us/sample - loss: 14542.5915 - val_loss: 16082.1974\n",
      "Epoch 939/1000\n",
      "2304/2304 [==============================] - 1s 509us/sample - loss: 14969.8064 - val_loss: 14207.3504\n",
      "Epoch 940/1000\n",
      "2304/2304 [==============================] - 1s 573us/sample - loss: 14614.6980 - val_loss: 12947.8037\n",
      "Epoch 941/1000\n",
      "2304/2304 [==============================] - 1s 532us/sample - loss: 14871.5971 - val_loss: 13728.1806\n",
      "Epoch 942/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 14401.4642 - val_loss: 17800.6878\n",
      "Epoch 943/1000\n",
      "2304/2304 [==============================] - 1s 522us/sample - loss: 14907.7212 - val_loss: 13305.3694\n",
      "Epoch 944/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 14759.1094 - val_loss: 13252.1360\n",
      "Epoch 945/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 14556.8617 - val_loss: 17824.6943\n",
      "Epoch 946/1000\n",
      "2304/2304 [==============================] - 1s 497us/sample - loss: 14562.1828 - val_loss: 13589.4432\n",
      "Epoch 947/1000\n",
      "2304/2304 [==============================] - 1s 508us/sample - loss: 14899.3534 - val_loss: 13239.5748\n",
      "Epoch 948/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 14005.6683 - val_loss: 13131.9392\n",
      "Epoch 949/1000\n",
      "2304/2304 [==============================] - 1s 503us/sample - loss: 14244.3025 - val_loss: 14020.0293\n",
      "Epoch 950/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 14377.8507 - val_loss: 15572.7715\n",
      "Epoch 951/1000\n",
      "2304/2304 [==============================] - 1s 512us/sample - loss: 15239.3423 - val_loss: 14181.5790\n",
      "Epoch 952/1000\n",
      "2304/2304 [==============================] - 1s 507us/sample - loss: 14169.1872 - val_loss: 13074.6265\n",
      "Epoch 953/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 15319.7367 - val_loss: 14339.6470\n",
      "Epoch 954/1000\n",
      "2304/2304 [==============================] - 1s 508us/sample - loss: 14184.8948 - val_loss: 13483.4173\n",
      "Epoch 955/1000\n",
      "2304/2304 [==============================] - 1s 505us/sample - loss: 14634.6820 - val_loss: 13029.4277\n",
      "Epoch 956/1000\n",
      "2304/2304 [==============================] - 1s 507us/sample - loss: 14127.3100 - val_loss: 13493.4420\n",
      "Epoch 957/1000\n",
      "2304/2304 [==============================] - 1s 575us/sample - loss: 14578.9353 - val_loss: 13178.2214\n",
      "Epoch 958/1000\n",
      "2304/2304 [==============================] - 1s 605us/sample - loss: 14648.6982 - val_loss: 13421.8537\n",
      "Epoch 959/1000\n",
      "2304/2304 [==============================] - 1s 505us/sample - loss: 14044.3829 - val_loss: 14857.3486\n",
      "Epoch 960/1000\n",
      "2304/2304 [==============================] - 1s 507us/sample - loss: 14775.8280 - val_loss: 14383.4356\n",
      "Epoch 961/1000\n",
      "2304/2304 [==============================] - 1s 504us/sample - loss: 13928.1905 - val_loss: 13288.5921\n",
      "Epoch 962/1000\n",
      "2304/2304 [==============================] - 1s 502us/sample - loss: 13939.0659 - val_loss: 13335.3522\n",
      "Epoch 963/1000\n",
      "2304/2304 [==============================] - 1s 526us/sample - loss: 14327.5158 - val_loss: 12999.2982\n",
      "Epoch 964/1000\n",
      "2304/2304 [==============================] - 1s 505us/sample - loss: 14249.0293 - val_loss: 16265.3707\n",
      "Epoch 965/1000\n",
      "2304/2304 [==============================] - 1s 501us/sample - loss: 14354.7858 - val_loss: 14236.9497\n",
      "Epoch 966/1000\n",
      "2304/2304 [==============================] - 1s 505us/sample - loss: 14144.2360 - val_loss: 12860.7526\n",
      "Epoch 967/1000\n",
      "2304/2304 [==============================] - 1s 506us/sample - loss: 14497.3355 - val_loss: 13715.3029\n",
      "Epoch 968/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 1s 495us/sample - loss: 13910.0242 - val_loss: 13908.5114\n",
      "Epoch 969/1000\n",
      "2304/2304 [==============================] - 1s 480us/sample - loss: 13987.1072 - val_loss: 13016.7265\n",
      "Epoch 970/1000\n",
      "2304/2304 [==============================] - 1s 477us/sample - loss: 14698.7401 - val_loss: 15550.0537\n",
      "Epoch 971/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 14238.1317 - val_loss: 13098.9740\n",
      "Epoch 972/1000\n",
      "2304/2304 [==============================] - 1s 480us/sample - loss: 15187.1361 - val_loss: 13679.0619\n",
      "Epoch 973/1000\n",
      "2304/2304 [==============================] - 1s 488us/sample - loss: 13999.7697 - val_loss: 13163.5376\n",
      "Epoch 974/1000\n",
      "2304/2304 [==============================] - 1s 474us/sample - loss: 13923.4850 - val_loss: 15796.0157\n",
      "Epoch 975/1000\n",
      "2304/2304 [==============================] - 1s 481us/sample - loss: 14536.9959 - val_loss: 13401.4796\n",
      "Epoch 976/1000\n",
      "2304/2304 [==============================] - 1s 475us/sample - loss: 14598.7542 - val_loss: 12929.1654\n",
      "Epoch 977/1000\n",
      "2304/2304 [==============================] - 1s 482us/sample - loss: 14733.0259 - val_loss: 14590.6228\n",
      "Epoch 978/1000\n",
      "2304/2304 [==============================] - 1s 515us/sample - loss: 14424.2768 - val_loss: 15083.3512\n",
      "Epoch 979/1000\n",
      "2304/2304 [==============================] - 1s 483us/sample - loss: 15841.2582 - val_loss: 17061.9283\n",
      "Epoch 980/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 14215.1444 - val_loss: 14090.5155\n",
      "Epoch 981/1000\n",
      "2304/2304 [==============================] - 1s 482us/sample - loss: 14182.9145 - val_loss: 12957.0904\n",
      "Epoch 982/1000\n",
      "2304/2304 [==============================] - 1s 483us/sample - loss: 14758.1915 - val_loss: 13313.3211\n",
      "Epoch 983/1000\n",
      "2304/2304 [==============================] - 1s 483us/sample - loss: 14280.2296 - val_loss: 12991.2681\n",
      "Epoch 984/1000\n",
      "2304/2304 [==============================] - 1s 483us/sample - loss: 14088.7411 - val_loss: 14255.8930\n",
      "Epoch 985/1000\n",
      "2304/2304 [==============================] - 1s 477us/sample - loss: 14137.4291 - val_loss: 13363.8300\n",
      "Epoch 986/1000\n",
      "2304/2304 [==============================] - 1s 481us/sample - loss: 14503.1373 - val_loss: 12675.6753\n",
      "Epoch 987/1000\n",
      "2304/2304 [==============================] - 1s 484us/sample - loss: 14487.1395 - val_loss: 14067.5735\n",
      "Epoch 988/1000\n",
      "2304/2304 [==============================] - 1s 479us/sample - loss: 14601.5794 - val_loss: 13193.3269\n",
      "Epoch 989/1000\n",
      "2304/2304 [==============================] - 1s 510us/sample - loss: 14136.7460 - val_loss: 13635.1792\n",
      "Epoch 990/1000\n",
      "2304/2304 [==============================] - 1s 536us/sample - loss: 13809.8092 - val_loss: 13131.4310\n",
      "Epoch 991/1000\n",
      "2304/2304 [==============================] - 1s 517us/sample - loss: 14385.2210 - val_loss: 14588.1977\n",
      "Epoch 992/1000\n",
      "2304/2304 [==============================] - 1s 498us/sample - loss: 14191.9718 - val_loss: 12847.1780\n",
      "Epoch 993/1000\n",
      "2304/2304 [==============================] - 1s 596us/sample - loss: 13840.9846 - val_loss: 13122.4860\n",
      "Epoch 994/1000\n",
      "2304/2304 [==============================] - 1s 476us/sample - loss: 15053.1251 - val_loss: 13084.9246\n",
      "Epoch 995/1000\n",
      "2304/2304 [==============================] - 1s 483us/sample - loss: 14085.8205 - val_loss: 15531.5265\n",
      "Epoch 996/1000\n",
      "2304/2304 [==============================] - 1s 485us/sample - loss: 14078.6303 - val_loss: 13357.9303\n",
      "Epoch 997/1000\n",
      "2304/2304 [==============================] - 1s 482us/sample - loss: 14267.3961 - val_loss: 13709.4740\n",
      "Epoch 998/1000\n",
      "2304/2304 [==============================] - 1s 476us/sample - loss: 14843.7149 - val_loss: 13828.5394\n",
      "Epoch 999/1000\n",
      "2304/2304 [==============================] - 1s 495us/sample - loss: 13903.5953 - val_loss: 13483.6511\n",
      "Epoch 1000/1000\n",
      "2304/2304 [==============================] - 1s 556us/sample - loss: 14270.2772 - val_loss: 13388.0513\n"
     ]
    }
   ],
   "source": [
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 50, kernel_initializer = 'he_uniform',activation='relu',input_dim = 174))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 25, kernel_initializer = 'he_uniform',activation='relu'))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(units = 50, kernel_initializer = 'he_uniform',activation='relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'he_uniform'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(loss=root_mean_squared_error, optimizer='Adamax')\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model_history=classifier.fit(X_train.values, y_train.values,validation_split=0.20, batch_size = 10, nb_epoch = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred=classifier.predict(df_Test.drop(['SalePrice'],axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create Sample Submission file and Submit\n",
    "pred1=pd.DataFrame(ann_pred)\n",
    "sub_df=pd.read_csv('datasets/sample_submission.csv')\n",
    "datasets=pd.concat([sub_df['Id'],pred1],axis=1)\n",
    "datasets.columns=['Id','SalePrice']\n",
    "datasets.to_csv('sample_submissionv2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
